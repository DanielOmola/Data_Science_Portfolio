{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - Load the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.ml.Pipeline\n",
       "import org.apache.spark.ml.feature.StringIndexer\n",
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "import org.apache.spark.ml.classification.DecisionTreeClassificationModel\n",
       "import org.apache.spark.ml.classification.DecisionTreeClassifier\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "import spark.implicits._\n",
       "import org.apache.spark.ml.feature.VectorIndexer\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.classification.DecisionTreeClassificationModel\n",
    "import org.apache.spark.ml.classification.DecisionTreeClassifier\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import spark.implicits._\n",
    "import org.apache.spark.ml.feature.VectorIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "/*!wget \"http://webia.lip6.fr/~baazizi/tc/fc/psl/20/data/DTClass.zip\"\n",
    "%cd\n",
    "!unzip DTClass.zip\n",
    "!ls -hal DTClass\n",
    "!cd DTClass;\n",
    "!ls /home/moi/Decision_Tree*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Start a Spark session and load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@11b2e092\n",
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    ".builder\n",
    ".appName(\"credit\")\n",
    ".config(\"spark.master\", \"local\")\n",
    ".getOrCreate()\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+-------------+-----+\n",
      "|   age|income|student|credit_rating|label|\n",
      "+------+------+-------+-------------+-----+\n",
      "| young|  high|     no|         fair|   no|\n",
      "| young|  high|     no|    excellent|   no|\n",
      "|middle|  high|     no|         fair|  yes|\n",
      "|senior|medium|     no|         fair|  yes|\n",
      "|senior|   low|    yes|         fair|  yes|\n",
      "|senior|   low|    yes|    excellent|   no|\n",
      "|middle|   low|    yes|    excellent|  yes|\n",
      "| young|medium|     no|         fair|   no|\n",
      "| young|   low|    yes|         fair|  yes|\n",
      "|senior|medium|    yes|         fair|  yes|\n",
      "| young|medium|    yes|    excellent|  yes|\n",
      "|middle|medium|     no|    excellent|  yes|\n",
      "|middle|  high|    yes|         fair|  yes|\n",
      "|senior|medium|     no|    excellent|   no|\n",
      "+------+------+-------+-------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "workingDir: String = data/\n",
       "data: org.apache.spark.sql.DataFrame = [age: string, income: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val workingDir = \"data/\"\n",
    "val data = spark.read.format(\"csv\").option(\"header\",\"true\").load(workingDir+\"credit.csv\")\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Features indexation, Target indexation, Training/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Target indexation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label: String = label\n",
       "labelIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_0661e82caa8d\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// ##### index the label attribute\n",
    "val label = \"label\"\n",
    "\n",
    "val labelIndexer = new StringIndexer()\n",
    "    .setInputCol(label)\n",
    "    .setOutputCol(\"indexed_\" + label) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Features String encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "attributes: Array[String] = Array(age, income, student, credit_rating)\n",
       "catFeatIndexer: Array[org.apache.spark.ml.feature.StringIndexer] = Array(strIdx_c3d4e1a7cea4, strIdx_0c2df3884339, strIdx_b8136b405779, strIdx_6e9afff0a6c5)\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val attributes= data.columns.filterNot(_.contains(label))\n",
    "\n",
    "val catFeatIndexer= attributes.map{\n",
    "    att => \n",
    "    new StringIndexer()\n",
    "    .setInputCol(att)\n",
    "    .setOutputCol(\"indexed_\" + att)   \n",
    "} \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Fit the string indexer to the data and extract the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "indcatFeatIndexer: Array[Array[(String, Int)]] = Array(Array((senior,0), (young,1), (middle,2)), Array((medium,0), (high,1), (low,2)), Array((no,0), (yes,1)), Array((fair,0), (excellent,1)))\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val indcatFeatIndexer = catFeatIndexer.map(x=>x.fit(data).labels.zipWithIndex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 - Assemble the indexed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "features: Array[String] = Array(indexed_age, indexed_income, indexed_student, indexed_credit_rating)\n",
       "vectorAssemb: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_a5116cf48202, handleInvalid=error, numInputCols=4\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val features = catFeatIndexer.map(_.getOutputCol)\n",
    "\n",
    "val vectorAssemb = new VectorAssembler()\n",
    ".setInputCols(features)\n",
    ".setOutputCol(\"assembled\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 - Index the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "maxCat: Int = 4\n",
       "vecIndexer: org.apache.spark.ml.feature.VectorIndexer = vecIdx_0bfcdc13b472\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val maxCat = 4\n",
    "val vecIndexer = new VectorIndexer()\n",
    ".setInputCol(vectorAssemb.getOutputCol)\n",
    ".setOutputCol(\"features\")\n",
    ".setMaxCategories(maxCat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Build and fit the pipeline on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_176af8fbb618\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline = new Pipeline()\n",
    ".setStages(Array(labelIndexer)++catFeatIndexer++Array(vectorAssemb,vecIndexer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ftdata: org.apache.spark.sql.DataFrame = [features: vector, indexed_label: double]\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ftdata = pipeline.fit(data)\n",
    "                         .transform(data)\n",
    "                         .select(\"features\",\"indexed_label\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- indexed_label: double (nullable = false)\n",
      "\n",
      "+-----------------+-------------+\n",
      "|         features|indexed_label|\n",
      "+-----------------+-------------+\n",
      "|[1.0,1.0,0.0,0.0]|          1.0|\n",
      "|[1.0,1.0,0.0,1.0]|          1.0|\n",
      "|[2.0,1.0,0.0,0.0]|          0.0|\n",
      "|        (4,[],[])|          0.0|\n",
      "|[0.0,2.0,1.0,0.0]|          0.0|\n",
      "|[0.0,2.0,1.0,1.0]|          1.0|\n",
      "|[2.0,2.0,1.0,1.0]|          0.0|\n",
      "|    (4,[0],[1.0])|          1.0|\n",
      "|[1.0,2.0,1.0,0.0]|          0.0|\n",
      "|    (4,[2],[1.0])|          0.0|\n",
      "|[1.0,0.0,1.0,1.0]|          0.0|\n",
      "|[2.0,0.0,0.0,1.0]|          0.0|\n",
      "|[2.0,1.0,1.0,0.0]|          0.0|\n",
      "|    (4,[3],[1.0])|          1.0|\n",
      "+-----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ftdata.printSchema\n",
    "ftdata.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainingData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [features: vector, indexed_label: double]\n",
       "testData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [features: vector, indexed_label: double]\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(trainingData,testData)= ftdata.randomSplit(Array(0.7,0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Decision Tree : build, train, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned classification tree model :\n",
      " DecisionTreeClassificationModel: uid=dtc_2fcb9be31b62, depth=3, numNodes=7, numClasses=2, numFeatures=4\n",
      "  If (feature 2 in {1.0})\n",
      "   Predict: 0.0\n",
      "  Else (feature 2 not in {1.0})\n",
      "   If (feature 0 in {2.0})\n",
      "    Predict: 0.0\n",
      "   Else (feature 0 not in {2.0})\n",
      "    If (feature 0 in {0.0})\n",
      "     Predict: 0.0\n",
      "    Else (feature 0 not in {0.0})\n",
      "     Predict: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dt: org.apache.spark.ml.classification.DecisionTreeClassifier = dtc_2fcb9be31b62\n",
       "dtModel: org.apache.spark.ml.classification.DecisionTreeClassificationModel = DecisionTreeClassificationModel: uid=dtc_2fcb9be31b62, depth=3, numNodes=7, numClasses=2, numFeatures=4\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dt = new DecisionTreeClassifier()\n",
    ".setLabelCol(\"indexed_label\")\n",
    ".setFeaturesCol(\"features\")\n",
    ".setMinInstancesPerNode(2)\n",
    "\n",
    "val dtModel = dt.fit(trainingData)\n",
    "print(s\"Learned classification tree model :\\n ${dtModel.toDebugString}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Model evaluation on Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------+-----------+----------+\n",
      "|         features|indexed_label|rawPrediction|probability|prediction|\n",
      "+-----------------+-------------+-------------+-----------+----------+\n",
      "|    (4,[0],[1.0])|          1.0|    [0.0,2.0]|  [0.0,1.0]|       1.0|\n",
      "|[0.0,2.0,1.0,0.0]|          0.0|    [4.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "|[0.0,2.0,1.0,1.0]|          1.0|    [4.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "|[2.0,2.0,1.0,1.0]|          0.0|    [4.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "+-----------------+-------------+-------------+-----------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.sql.DataFrame = [features: vector, indexed_label: double ... 3 more fields]\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions = dtModel.transform(testData)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function for performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "performance: (model: org.apache.spark.ml.classification.DecisionTreeClassificationModel, model_name: String)Any\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def performance(model : org.apache.spark.ml.classification.DecisionTreeClassificationModel,model_name : String): Any = {\n",
    "\n",
    "            val training_predictions = model.transform(trainingData)\n",
    "                                  .select(\"prediction\", \"indexed_label\")\n",
    "                                  .cache()\n",
    "\n",
    "            val test_predictions = model.transform(testData)\n",
    "                                  .select(\"prediction\", \"indexed_label\")\n",
    "                                   .cache()\n",
    "    \n",
    "            val predictions = Array(training_predictions, test_predictions)\n",
    "            val names = Array(\" Training \", \" Test \")\n",
    "    \n",
    "    println(s\"\\n##################### ${model_name} Performance #########################\")\n",
    "    \n",
    "    for (i <- 0 until predictions.length)\n",
    "    {        \n",
    "         var data = predictions(i)\n",
    "         var name = names(i)\n",
    "    \n",
    "            // Select (prediction, true label) and compute test error.\n",
    "            var evaluator = new MulticlassClassificationEvaluator()\n",
    "              .setLabelCol(\"indexed_label\")\n",
    "              .setPredictionCol(\"prediction\")\n",
    "              .setMetricName(\"accuracy\")\n",
    "\n",
    "            var accuracy = evaluator.evaluate(data)\n",
    "\n",
    "\n",
    "    \n",
    "            evaluator = new MulticlassClassificationEvaluator()\n",
    "              .setLabelCol(\"indexed_label\")\n",
    "              .setPredictionCol(\"prediction\")\n",
    "              .setMetricName(\"f1\")\n",
    "\n",
    "            var f1 = evaluator.evaluate(data)\n",
    "            \n",
    "            \n",
    "            println(s\"\\n---------------------- ${name} performance metrics----------------------\\n\")\n",
    "            println(s\"\\t- Accuracy = ${(accuracy * 100)}\")\n",
    "            println(s\"\\t- Error = ${(1.0 - accuracy)*100}\")\n",
    "            println(s\"\\t- F1 score = ${(f1)}\")\n",
    "    }\n",
    "      \n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##################### Decision Tree Performance #########################\n",
      "\n",
      "----------------------  Training  performance metrics----------------------\n",
      "\n",
      "\t- Accuracy = 90.0\n",
      "\t- Error = 9.999999999999998\n",
      "\t- F1 score = 0.8933333333333333\n",
      "\n",
      "----------------------  Test  performance metrics----------------------\n",
      "\n",
      "\t- Accuracy = 75.0\n",
      "\t- Error = 25.0\n",
      "\t- F1 score = 0.7333333333333334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res32: Any = ()\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance(dtModel,\"Decision Tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class org.apache.spark.ml.classification.DecisionTreeClassificationModel\n"
     ]
    }
   ],
   "source": [
    "println(dtModel.getClass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+-------------+\n",
      "|   age|income|student|credit_rating|\n",
      "+------+------+-------+-------------+\n",
      "| young|  high|     no|         fair|\n",
      "|senior|  high|    yes|    excellent|\n",
      "+------+------+-------+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "test_df: org.apache.spark.sql.DataFrame = [age: string, income: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val test_df = Seq((\"young\",\"high\",\"no\",\"fair\"),\n",
    "                 (\"senior\",\"high\",\"yes\",\"excellent\"))\n",
    ".toDF(\"age\",\"income\",\"student\",\"credit_rating\")\n",
    "\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictionPipeline: org.apache.spark.ml.Pipeline = pipeline_f99a232c44c9\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictionPipeline = new Pipeline().setStages(pipeline.getStages.slice(1,pipeline.getStages.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|         features|\n",
      "+-----------------+\n",
      "|[1.0,0.0,0.0,1.0]|\n",
      "|    (4,[2],[1.0])|\n",
      "+-----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "test_data: org.apache.spark.sql.DataFrame = [features: vector]\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val test_data = predictionPipeline.fit(test_df).transform(test_df).select(\"features\")\n",
    "test_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-----------+----------+\n",
      "|         features|rawPrediction|probability|prediction|\n",
      "+-----------------+-------------+-----------+----------+\n",
      "|[1.0,0.0,0.0,1.0]|    [0.0,2.0]|  [0.0,1.0]|       1.0|\n",
      "|    (4,[2],[1.0])|    [3.0,0.0]|  [1.0,0.0]|       0.0|\n",
      "+-----------------+-------------+-----------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.sql.DataFrame = [features: vector, rawPrediction: vector ... 2 more fields]\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions = dtModel.transform(test_data)\n",
    "predictions.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
