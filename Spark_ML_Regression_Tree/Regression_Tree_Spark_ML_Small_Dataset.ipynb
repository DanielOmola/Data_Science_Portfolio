{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - Load the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, StringIndexerModel, VectorAssembler, VectorIndexer}\n",
       "import org.apache.spark.ml.Pipeline\n",
       "import org.apache.spark.ml.regression.DecisionTreeRegressionModel\n",
       "import org.apache.spark.ml.regression.DecisionTreeRegressor\n",
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, StringIndexerModel, VectorAssembler, VectorIndexer}\n",
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "import org.apache.spark.ml.regression.DecisionTreeRegressionModel\n",
    "import org.apache.spark.ml.regression.DecisionTreeRegressor\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Start a Spark session and load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+--------+-----+-----+\n",
      "| outlook|temp|humidity|windy|hours|\n",
      "+--------+----+--------+-----+-----+\n",
      "|   rainy| hot|    high|FALSE|   25|\n",
      "|   rainy| hot|    high| TRUE|   30|\n",
      "|overcast| hot|    high|FALSE|   46|\n",
      "|   sunny|mild|    high|FALSE|   45|\n",
      "|   sunny|cool|  normal|FALSE|   52|\n",
      "|   sunny|cool|  normal| TRUE|   23|\n",
      "|overcast|cool|  normal| TRUE|   43|\n",
      "|   rainy|mild|    high|FALSE|   35|\n",
      "|   rainy|cool|  normal|FALSE|   38|\n",
      "|   sunny|mild|  normal|FALSE|   46|\n",
      "|   rainy|mild|  normal| TRUE|   48|\n",
      "|overcast|mild|    high| TRUE|   52|\n",
      "|overcast| hot|  normal|FALSE|   44|\n",
      "|   sunny|mild|    high| TRUE|   30|\n",
      "+--------+----+--------+-----+-----+\n",
      "\n",
      "root\n",
      " |-- outlook: string (nullable = true)\n",
      " |-- temp: string (nullable = true)\n",
      " |-- humidity: string (nullable = true)\n",
      " |-- windy: string (nullable = true)\n",
      " |-- hours: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@17ed7250\n",
       "import spark.implicits._\n",
       "workingDir: String = data/\n",
       "data: org.apache.spark.sql.DataFrame = [outlook: string, temp: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// ##### Start Spark session\n",
    "val spark = SparkSession\n",
    ".builder\n",
    ".appName(\"auto\")\n",
    ".config(\"spark.master\", \"local\")\n",
    ".getOrCreate()\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "\n",
    "// ##### load data\n",
    "val workingDir = \"data/\"\n",
    "val data = spark.read.format(\"csv\").option(\"header\",\"true\").load(workingDir+\"regtree.csv\")\n",
    "data.show()\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Features indexation, Target indexation, Training/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Target indexation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label: String = hours\n",
       "labelIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_26389e8035fb\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// ##### index the label attribute */\n",
    "val label = \"hours\"\n",
    "\n",
    "val labelIndexer = new StringIndexer()\n",
    "    .setInputCol(label)\n",
    "    .setOutputCol(\"indexed_\" + label) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Features String encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "attributes: Array[String] = Array(outlook, temp, humidity, windy)\n",
       "catFeatIndexer: Array[org.apache.spark.ml.feature.StringIndexer] = Array(strIdx_402e3f9f83a1, strIdx_8ce220626c0b, strIdx_6dc2640d133c, strIdx_ee7f4de8d18a)\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val attributes= data.columns.filterNot(_.contains(label))\n",
    "\n",
    "val catFeatIndexer= attributes.map{\n",
    "    att => \n",
    "    new StringIndexer()\n",
    "    .setInputCol(att)\n",
    "    .setOutputCol(\"indexed_\" + att)   \n",
    "} \n",
    "\n",
    "//println(attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Fit the string indexer to the data and extract the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "indcatFeatIndexer: Array[Array[(String, Int)]] = Array(Array((rainy,0), (sunny,1), (overcast,2)), Array((mild,0), (cool,1), (hot,2)), Array((high,0), (normal,1)), Array((FALSE,0), (TRUE,1)))\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val indcatFeatIndexer = catFeatIndexer.map(x=>x.fit(data).labels.zipWithIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "attributes: Array[String] = Array(outlook, temp, humidity, windy)\n",
       "catFeatIndexer: Array[org.apache.spark.ml.feature.StringIndexer] = Array(strIdx_6515ff294c1b, strIdx_605404e8b5bc, strIdx_6d46ce77927f, strIdx_4775172b654e)\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*val attributes= data.columns.filterNot(_.contains(label))\n",
    "val catFeatIndexer= attributes.map{\n",
    "    att => \n",
    "    new StringIndexer()\n",
    "    .setInputCol(att)\n",
    "    .setOutputCol(\"indexed_\" + att)   \n",
    "} */\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "att_outlook: String = outlook\n",
       "outlook_indexer: org.apache.spark.ml.feature.StringIndexerModel = strIdx_f23748d4e994\n",
       "res12: Array[(String, Int)] = Array((rainy,0), (sunny,1), (overcast,2))\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*val att_outlook = \"outlook\"\n",
    "val outlook_indexer = new StringIndexer()\n",
    "    .setInputCol(att_outlook)\n",
    "    .setOutputCol(\"indexed_\" + att) \n",
    "    .fit(data)\n",
    "\n",
    "\n",
    "outlook_indexer.labels.zipWithIndex*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "att_temp: String = temp\n",
       "temp_indexer: org.apache.spark.ml.feature.StringIndexerModel = strIdx_52fdaf4e6fef\n",
       "res18: Array[(String, Int)] = Array((mild,0), (cool,1), (hot,2))\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*val att_temp = \"temp\"\n",
    "val temp_indexer = new StringIndexer()\n",
    "    .setInputCol(att_temp)\n",
    "    .setOutputCol(\"indexed_\" + att) \n",
    "    .fit(data)\n",
    "\n",
    "//val data_1 = temp_indexer.fit(data).transform(data)\n",
    "\n",
    "temp_indexer.labels.zipWithIndex*/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "att_hum: String = humidity\n",
       "hum_indexer: org.apache.spark.ml.feature.StringIndexerModel = strIdx_7dbac1adf9f0\n",
       "res10: Array[(String, Int)] = Array((high,0), (normal,1))\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*val att_hum = \"humidity\"\n",
    "val hum_indexer = new StringIndexer()\n",
    "    .setInputCol(att_hum)\n",
    "    .setOutputCol(\"indexed_\" + att) \n",
    "    .fit(temp_indexer)\n",
    "hum_indexer.labels.zipWithIndex*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "att: String = humidity\n",
       "hstrmodel: org.apache.spark.ml.feature.StringIndexerModel = strIdx_384e7db14bb4\n",
       "res4: Array[(String, Int)] = Array((high,0), (normal,1))\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*val att = \"windy\"\n",
    "val hstrmodel = new StringIndexer()\n",
    "    .setInputCol(att)\n",
    "    .setOutputCol(\"indexed_\" + att) \n",
    "    .fit(data)\n",
    "hstrmodel.labels.zipWithIndex*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 - Assemble the indexed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "features: Array[String] = Array(indexed_outlook, indexed_temp, indexed_humidity, indexed_windy)\n",
       "vectorAssemb: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_271c434a6e40, handleInvalid=error, numInputCols=4\n",
       "maxCat: Int = 4\n",
       "vecIndexer: org.apache.spark.ml.feature.VectorIndexer = vecIdx_60631d41fc12\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val features = catFeatIndexer.map(_.getOutputCol)\n",
    "\n",
    "// assemble with the rest of the features\n",
    "val vectorAssemb = new VectorAssembler()\n",
    ".setInputCols(features)\n",
    ".setOutputCol(\"assembled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 - Index the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// index the vector\n",
    "val maxCat = 4\n",
    "val vecIndexer = new VectorIndexer()\n",
    ".setInputCol(vectorAssemb.getOutputCol)\n",
    ".setOutputCol(\"features\")\n",
    ".setMaxCategories(maxCat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Build and fit the pipeline on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_fdd1b3ec06cf\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline = new Pipeline()\n",
    ".setStages(Array(labelIndexer)++catFeatIndexer++Array(vectorAssemb,vecIndexer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types.DoubleType\n",
       "ftdata: org.apache.spark.sql.DataFrame = [features: vector, label: double]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.DoubleType\n",
    "val ftdata = pipeline.fit(data).transform(data)\n",
    ".withColumn(\"label\",col(label).cast(DoubleType))\n",
    ".select(\"features\",\"label\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n",
      "+-----------------+-----+\n",
      "|         features|label|\n",
      "+-----------------+-----+\n",
      "|    (4,[1],[2.0])| 25.0|\n",
      "|[0.0,2.0,0.0,1.0]| 30.0|\n",
      "|[2.0,2.0,0.0,0.0]| 46.0|\n",
      "|    (4,[0],[1.0])| 45.0|\n",
      "|[1.0,1.0,1.0,0.0]| 52.0|\n",
      "|[1.0,1.0,1.0,1.0]| 23.0|\n",
      "|[2.0,1.0,1.0,1.0]| 43.0|\n",
      "|        (4,[],[])| 35.0|\n",
      "|[0.0,1.0,1.0,0.0]| 38.0|\n",
      "|[1.0,0.0,1.0,0.0]| 46.0|\n",
      "|[0.0,0.0,1.0,1.0]| 48.0|\n",
      "|[2.0,0.0,0.0,1.0]| 52.0|\n",
      "|[2.0,2.0,1.0,0.0]| 44.0|\n",
      "|[1.0,0.0,0.0,1.0]| 30.0|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ftdata.printSchema\n",
    "ftdata.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainingData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [features: vector, label: double]\n",
       "testData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [features: vector, label: double]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(trainingData,testData)= ftdata.randomSplit(Array(0.7,0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Decision Tree : build, train, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dt: org.apache.spark.ml.regression.DecisionTreeRegressor = dtr_f1ffe28775f8\n",
       "model: org.apache.spark.ml.regression.DecisionTreeRegressionModel = DecisionTreeRegressionModel: uid=dtr_f1ffe28775f8, depth=2, numNodes=7, numFeatures=4\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// train the decision tree\n",
    "val dt = new DecisionTreeRegressor()\n",
    ".setLabelCol(\"label\")\n",
    ".setFeaturesCol(\"features\")\n",
    ".setMinInstancesPerNode(2)\n",
    "\n",
    "val model = dt.fit(trainingData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned classification tree model :\n",
      " DecisionTreeRegressionModel: uid=dtr_f1ffe28775f8, depth=2, numNodes=7, numFeatures=4\n",
      "  If (feature 2 in {0.0})\n",
      "   If (feature 0 in {0.0})\n",
      "    Predict: 30.0\n",
      "   Else (feature 0 not in {0.0})\n",
      "    Predict: 40.333333333333336\n",
      "  Else (feature 2 not in {0.0})\n",
      "   If (feature 0 in {0.0})\n",
      "    Predict: 43.0\n",
      "   Else (feature 0 not in {0.0})\n",
      "    Predict: 47.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "treeModel: org.apache.spark.ml.regression.DecisionTreeRegressionModel = DecisionTreeRegressionModel: uid=dtr_f1ffe28775f8, depth=2, numNodes=7, numFeatures=4\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val treeModel=model.asInstanceOf[DecisionTreeRegressionModel]\n",
    "print(s\"Learned classification tree model :\\n ${treeModel.toDebugString}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (MSE) on test data = 13.700922515574554\n",
      "Root Mean Squared Error (MAE) on test data = 10.291666666666666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.sql.DataFrame = [features: vector, label: double ... 1 more field]\n",
       "evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = RegressionEvaluator: uid=regEval_8feebce69d33, metricName=mae, throughOrigin=false\n",
       "mse: Double = 13.700922515574554\n",
       "mae: Double = 10.291666666666666\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// ####### make predictions\n",
    "\n",
    "val predictions = model.transform(testData)\n",
    "\n",
    "// ####### Select (prediction, true label) and compute error\n",
    "\n",
    "val evaluator = new RegressionEvaluator()\n",
    ".setLabelCol(\"label\")\n",
    ".setPredictionCol(\"prediction\")\n",
    "\n",
    "// ####### get RMSE\n",
    "evaluator.setMetricName(\"rmse\")\n",
    "val mse = evaluator.evaluate(predictions)\n",
    "println(s\"Root Mean Squared Error (MSE) on test data = $mse\")\n",
    "\n",
    "// ####### get RMSE\n",
    "evaluator.setMetricName(\"mae\")\n",
    "val mae = evaluator.evaluate(predictions)\n",
    "println(s\"Root Mean Squared Error (MAE) on test data = $mae\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(humidity,0.5749234273148526)\n",
      "(outlook,0.42507657268514737)\n",
      "(temp,0.0)\n",
      "(windy,0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "featureImportances: org.apache.spark.ml.linalg.Vector = (4,[0,2],[0.42507657268514737,0.5749234273148526])\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val featureImportances = model.featureImportances\n",
    "val res = attributes.zip(featureImportances.toArray).sortBy(-_._2).foreach(println)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
