{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2227f332",
   "metadata": {},
   "source": [
    "# 0 - Load the modules and create Case Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae1794fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://0d06e632b0ca:4040\n",
       "SparkContext available as 'sc' (version = 3.1.1, master = local[*], app id = local-1621251557891)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.types._\n",
       "import spark.sqlContext.implicits._\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import spark.sqlContext.implicits._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31de6e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class flight\n",
       "defined class weather\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/* ---------------------------------------------------  \n",
    "    Create Class flight and weather for later use\n",
    "-------------- -------------------------------------- */\n",
    "\n",
    "case class flight(\n",
    "                    ORIGIN : String,\n",
    "                    DEST : String,\n",
    "                    Tad:String,\n",
    "                    Tsd:String,\n",
    "                    Taa:String,\n",
    "                    Tsa:String,\n",
    "                    Th:Boolean)\n",
    "\n",
    "case class weather(\n",
    "                    t0: String,\n",
    "                     A: String, \n",
    "                     H: Float,\n",
    "                     Wd: Float,\n",
    "                     Ws: Float,\n",
    "                     P: Float,\n",
    "                     T6: Float,\n",
    "                     S: String, \n",
    "                     D: String, \n",
    "                     V: Float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458f316a",
   "metadata": {},
   "source": [
    "# 1 - Create a Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83fd0ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@569230f6\n",
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/* ------------------------------------------  \n",
    "    Create SparkSession\n",
    "-------------- ---------------------------- */\n",
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession\n",
    ".builder\n",
    ".appName(\"flight\")\n",
    ".config(\"spark.master\", \"local\")\n",
    ".getOrCreate()\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fec53e",
   "metadata": {},
   "source": [
    "# 2 - Create Flight dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "577be4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-------------------+-------------------+-------------------+-------------------+-----+\n",
      "|ORIGIN|DEST|                Tad|                Tsd|                Taa|                Tsa|   Th|\n",
      "+------+----+-------------------+-------------------+-------------------+-------------------+-----+\n",
      "|   PDX| IAH|2012-01-01 05:59:00|2012-01-01 06:00:00|2012-01-01 11:42:00|2012-01-01 12:02:00|false|\n",
      "|   AUS| LAX|2012-01-01 05:59:00|2012-01-01 06:03:00|2012-01-01 07:24:00|2012-01-01 07:24:00|false|\n",
      "|   TPA| PHL|2012-01-01 06:05:00|2012-01-01 06:10:00|2012-01-01 08:23:00|2012-01-01 08:35:00|false|\n",
      "|   PDX| DEN|2012-01-01 06:05:00|2012-01-01 06:15:00|2012-01-01 09:20:00|2012-01-01 09:40:00|false|\n",
      "|   RNO| PHX|2012-01-01 06:08:00|2012-01-01 06:15:00|2012-01-01 08:44:00|2012-01-01 08:58:00|false|\n",
      "+------+----+-------------------+-------------------+-------------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "workingDir: String = data/\n",
       "FT: org.apache.spark.sql.Dataset[flight] = [ORIGIN: string, DEST: string ... 5 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/* ---------------------------------------------------  \n",
    "            Create a FT dataframe\n",
    "    with flights data and flight case class\n",
    "-------------- -------------------------------------- */\n",
    "\n",
    "val workingDir = \"data/\"\n",
    "val FT = spark.read.format(\"csv\")\n",
    ".option(\"header\",\"true\")\n",
    ".option(\"inferSchema\",\"true\")\n",
    ".load(workingDir+\"flight_1M.csv\")\n",
    ".as[flight]\n",
    "FT.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5c082b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- Tad_unix: long (nullable = true)\n",
      " |-- Tad_unix_12: long (nullable = true)\n",
      " |-- Tsd_unix: long (nullable = true)\n",
      " |-- Tsd_unix_1: long (nullable = true)\n",
      " |-- Tsd_unix_2: long (nullable = true)\n",
      " |-- Tsd_unix_3: long (nullable = true)\n",
      " |-- Tsd_unix_4: long (nullable = true)\n",
      " |-- Tsd_unix_5: long (nullable = true)\n",
      " |-- Tsd_unix_6: long (nullable = true)\n",
      " |-- Tsd_unix_7: long (nullable = true)\n",
      " |-- Tsd_unix_8: long (nullable = true)\n",
      " |-- Tsd_unix_9: long (nullable = true)\n",
      " |-- Tsd_unix_10: long (nullable = true)\n",
      " |-- Tsd_unix_11: long (nullable = true)\n",
      " |-- Tsd_unix_12: long (nullable = true)\n",
      " |-- Taa_unix: long (nullable = true)\n",
      " |-- Taa_unix_12: long (nullable = true)\n",
      " |-- Tsa_unix: long (nullable = true)\n",
      " |-- Tsa_unix_1: long (nullable = true)\n",
      " |-- Tsa_unix_2: long (nullable = true)\n",
      " |-- Tsa_unix_3: long (nullable = true)\n",
      " |-- Tsa_unix_4: long (nullable = true)\n",
      " |-- Tsa_unix_5: long (nullable = true)\n",
      " |-- Tsa_unix_6: long (nullable = true)\n",
      " |-- Tsa_unix_7: long (nullable = true)\n",
      " |-- Tsa_unix_8: long (nullable = true)\n",
      " |-- Tsa_unix_9: long (nullable = true)\n",
      " |-- Tsa_unix_10: long (nullable = true)\n",
      " |-- Tsa_unix_11: long (nullable = true)\n",
      " |-- Tsa_unix_12: long (nullable = true)\n",
      " |-- Th: boolean (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data_f: org.apache.spark.sql.DataFrame = [ORIGIN: string, DEST: string ... 31 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "import spark.sqlContext.implicits._*/\n",
    "\n",
    "/* ---------------------------------------------------  \n",
    "    add hourly time slots starting 12 hours before the\n",
    "    scheduled departure/arrival time by converting \n",
    "    scheduled departure date and schedule arrival date\n",
    "    to unix timestamp for Flight dataframe. \n",
    "    \n",
    "    The conversion to unix timestamp is also done for\n",
    "    the scheduled departure/arrival time\n",
    "-------------- -------------------------------------- */\n",
    "\n",
    "val data_f = FT.select(\n",
    "    $\"ORIGIN\",\n",
    "    $\"DEST\",\n",
    "    //$\"Tad\",\n",
    "    unix_timestamp($\"Tad\",\"yyyy-MM-dd HH:mm:ss\").as(\"Tad_unix\"),\n",
    "    (unix_timestamp($\"Tad\",\"yyyy-MM-dd HH:mm:ss\")-12*3600).as(\"Tad_unix_12\"),\n",
    "    //$\"Tsd\",\n",
    "    unix_timestamp($\"Tsd\",\"yyyy-MM-dd HH:mm:ss\").as(\"Tsd_unix\"),\n",
    "    \n",
    "    // partie ajout√©e FP 30 Juin ///////////////////////////////////////////////////////////////////////\n",
    "    (unix_timestamp($\"Tsd\",\"yyyy-MM-dd HH:mm:ss\")-1*3600).as(\"Tsd_unix_1\"),\n",
    "    (unix_timestamp($\"Tsd\",\"yyyy-MM-dd HH:mm:ss\")-2*3600).as(\"Tsd_unix_2\"),\n",
    "    (unix_timestamp($\"Tsd\",\"yyyy-MM-dd HH:mm:ss\")-3*3600).as(\"Tsd_unix_3\"),\n",
    "    (unix_timestamp($\"Tsd\",\"yyyy-MM-dd HH:mm:ss\")-4*3600).as(\"Tsd_unix_4\"),\n",
    "    (unix_timestamp($\"Tsd\",\"yyyy-MM-dd HH:mm:ss\")-5*3600).as(\"Tsd_unix_5\"),\n",
    "    (unix_timestamp($\"Tsd\",\"yyyy-MM-dd HH:mm:ss\")-6*3600).as(\"Tsd_unix_6\"),\n",
    "    (unix_timestamp($\"Tsd\",\"yyyy-MM-dd HH:mm:ss\")-7*3600).as(\"Tsd_unix_7\"),\n",
    "    (unix_timestamp($\"Tsd\",\"yyyy-MM-dd HH:mm:ss\")-8*3600).as(\"Tsd_unix_8\"),\n",
    "    (unix_timestamp($\"Tsd\",\"yyyy-MM-dd HH:mm:ss\")-9*3600).as(\"Tsd_unix_9\"),\n",
    "    (unix_timestamp($\"Tsd\",\"yyyy-MM-dd HH:mm:ss\")-10*3600).as(\"Tsd_unix_10\"),\n",
    "    (unix_timestamp($\"Tsd\",\"yyyy-MM-dd HH:mm:ss\")-11*3600).as(\"Tsd_unix_11\"),\n",
    "    // fin ajout FP 30 Juin  ////////////////////////////////////////////////////////////////////////////\n",
    "    \n",
    "    (unix_timestamp($\"Tsd\",\"yyyy-MM-dd HH:mm:ss\")-12*3600).as(\"Tsd_unix_12\"),\n",
    "    //$\"Taa\",\n",
    "    unix_timestamp($\"Taa\",\"yyyy-MM-dd HH:mm:ss\").as(\"Taa_unix\"),\n",
    "    (unix_timestamp($\"Taa\",\"yyyy-MM-dd HH:mm:ss\")-12*3600).as(\"Taa_unix_12\"),\n",
    "    //$\"Tsa\",\n",
    "    unix_timestamp($\"Tsa\",\"yyyy-MM-dd HH:mm:ss\").as(\"Tsa_unix\"),\n",
    "    \n",
    "    // partie ajout√©e FP 30 Juin ///////////////////////////////////////////////////////////////////////\n",
    "    (unix_timestamp($\"Tsa\",\"yyyy-MM-dd HH:mm:ss\")-1*3600).as(\"Tsa_unix_1\"),\n",
    "    (unix_timestamp($\"Tsa\",\"yyyy-MM-dd HH:mm:ss\")-2*3600).as(\"Tsa_unix_2\"),\n",
    "    (unix_timestamp($\"Tsa\",\"yyyy-MM-dd HH:mm:ss\")-3*3600).as(\"Tsa_unix_3\"),\n",
    "    (unix_timestamp($\"Tsa\",\"yyyy-MM-dd HH:mm:ss\")-4*3600).as(\"Tsa_unix_4\"),\n",
    "    (unix_timestamp($\"Tsa\",\"yyyy-MM-dd HH:mm:ss\")-5*3600).as(\"Tsa_unix_5\"),\n",
    "    (unix_timestamp($\"Tsa\",\"yyyy-MM-dd HH:mm:ss\")-6*3600).as(\"Tsa_unix_6\"),\n",
    "    (unix_timestamp($\"Tsa\",\"yyyy-MM-dd HH:mm:ss\")-7*3600).as(\"Tsa_unix_7\"),\n",
    "    (unix_timestamp($\"Tsa\",\"yyyy-MM-dd HH:mm:ss\")-8*3600).as(\"Tsa_unix_8\"),\n",
    "    (unix_timestamp($\"Tsa\",\"yyyy-MM-dd HH:mm:ss\")-9*3600).as(\"Tsa_unix_9\"),\n",
    "    (unix_timestamp($\"Tsa\",\"yyyy-MM-dd HH:mm:ss\")-10*3600).as(\"Tsa_unix_10\"),\n",
    "    (unix_timestamp($\"Tsa\",\"yyyy-MM-dd HH:mm:ss\")-11*3600).as(\"Tsa_unix_11\"),\n",
    "    // fin ajout FP 30 Juin  ////////////////////////////////////////////////////////////////////////////\n",
    "    \n",
    "    (unix_timestamp($\"Tsa\",\"yyyy-MM-dd HH:mm:ss\")-12*3600).as(\"Tsa_unix_12\"),    \n",
    "   //concat($\"ORIGIN\",$\"Tad_unix\").as(\"originK\"),\n",
    "    //concat($\"DEST\",$\"Tsa_unix\").as(\"destK\"),\n",
    "    $\"Th\")\n",
    "\n",
    "\n",
    "data_f.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "911f56f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data_flights: org.apache.spark.sql.DataFrame = [ORIGIN: string, DEST: string ... 33 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/* ---------------------------------------------------  \n",
    "    Add origin key and arrival key columns\n",
    "    to Flight data frame and select the final columns\n",
    "-------------- -------------------------------------- */\n",
    "\n",
    "val data_flights = data_f.select(\n",
    "    $\"ORIGIN\",\n",
    "    $\"DEST\",\n",
    "    concat($\"ORIGIN\",$\"Tad_unix\").as(\"originK\"),\n",
    "    concat($\"DEST\",$\"Tsa_unix\").as(\"destK\"),\n",
    "    //$\"Tad\",\n",
    "    $\"Tad_unix\",\n",
    "    $\"Tad_unix_12\",\n",
    "    //$\"Tsd\",\n",
    "    $\"Tsd_unix\",\n",
    "    \n",
    "    // partie ajout√©e FP 30 Juin ///////////////////////////////////////////////////////////////////////\n",
    "    $\"Tsd_unix_1\",\n",
    "    $\"Tsd_unix_2\",\n",
    "    $\"Tsd_unix_3\",\n",
    "    $\"Tsd_unix_4\",\n",
    "    $\"Tsd_unix_5\",\n",
    "    $\"Tsd_unix_6\",\n",
    "    $\"Tsd_unix_7\",\n",
    "    $\"Tsd_unix_8\",\n",
    "    $\"Tsd_unix_9\",\n",
    "    $\"Tsd_unix_10\",\n",
    "    $\"Tsd_unix_11\",\n",
    "    // partie ajout√©e FP 30 Juin ///////////////////////////////////////////////////////////////////////\n",
    "    \n",
    "    $\"Tsd_unix_12\",\n",
    "    //$\"Taa\",\n",
    "    $\"Taa_unix\",\n",
    "    $\"Taa_unix_12\",\n",
    "    //$\"Tsa\",\n",
    "    $\"Tsa_unix\",\n",
    "    \n",
    "    // partie ajout√©e FP 30 Juin ///////////////////////////////////////////////////////////////////////\n",
    "    $\"Tsa_unix_1\",\n",
    "    $\"Tsa_unix_2\",\n",
    "    $\"Tsa_unix_3\",\n",
    "    $\"Tsa_unix_4\",\n",
    "    $\"Tsa_unix_5\",\n",
    "    $\"Tsa_unix_6\",\n",
    "    $\"Tsa_unix_7\",\n",
    "    $\"Tsa_unix_8\",\n",
    "    $\"Tsa_unix_9\",\n",
    "    $\"Tsa_unix_10\",\n",
    "    $\"Tsa_unix_11\",\n",
    "    // partie ajout√©e FP 30 Juin //////////////////////////////////////////////////////////////////////\n",
    "    \n",
    "    $\"Tsa_unix_12\",\n",
    "\n",
    "    $\"Th\")\n",
    "//data_flights.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7881cba2",
   "metadata": {},
   "source": [
    "# 3 - Create Weather dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57912664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+----+-----+----+-----+----+---+---+----+\n",
      "|                 t0|  A|   H|   Wd|  Ws|    P|  T6|  S|  D|   V|\n",
      "+-------------------+---+----+-----+----+-----+----+---+---+----+\n",
      "|2012-01-01 00:53:00|DEN|65.0|160.0|14.0| 24.8|11.0|CLR|   |10.0|\n",
      "|2012-01-01 01:53:00|DEN|57.0|150.0|15.0|24.79|10.0|CLR|   |10.0|\n",
      "|2012-01-01 02:53:00|DEN|53.0|150.0|14.0|24.81| 9.0|CLR|   |10.0|\n",
      "|2012-01-01 03:53:00|DEN|50.0|160.0|13.0|24.81| 8.0|CLR|   |10.0|\n",
      "|2012-01-01 04:53:00|DEN|52.0| null| 3.0|24.84| 8.0|CLR|   |10.0|\n",
      "+-------------------+---+----+-----+----+-----+----+---+---+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OT: org.apache.spark.sql.Dataset[weather] = [t0: string, A: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/* ---------------------------------------------------  \n",
    "            Create a OT dataframe\n",
    "    with weather data and weather case class\n",
    "-------------- -------------------------------------- */\n",
    "val OT = spark.read.format(\"csv\")\n",
    ".option(\"header\",\"true\")\n",
    ".option(\"inferSchema\",\"true\")\n",
    ".option(\"timestampFormat\", \"yyyy-MM-dd'T'HH:mm:ss\")\n",
    ".load(workingDir+\"weather_1M.csv\")\n",
    ".withColumn(\"H\",$\"H\".cast(\"float\"))\n",
    ".withColumn(\"Wd\",$\"Wd\".cast(\"float\"))\n",
    ".withColumn(\"Ws\",$\"Ws\".cast(\"float\"))\n",
    ".withColumn(\"P\",$\"P\".cast(\"float\"))\n",
    ".withColumn(\"T6\",$\"T6\".cast(\"float\"))\n",
    ".withColumn(\"V\",$\"V\".cast(\"float\"))\n",
    ".as[weather]\n",
    "OT.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c00539e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- t0: string (nullable = true)\n",
      " |-- t0_unix: long (nullable = true)\n",
      " |-- A: string (nullable = true)\n",
      " |-- H: float (nullable = true)\n",
      " |-- Wd: float (nullable = true)\n",
      " |-- Ws: float (nullable = true)\n",
      " |-- P: float (nullable = true)\n",
      " |-- T6: float (nullable = true)\n",
      " |-- S: string (nullable = true)\n",
      " |-- D: string (nullable = true)\n",
      " |-- V: float (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data_w: org.apache.spark.sql.DataFrame = [t0: string, t0_unix: bigint ... 9 more fields]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//import spark.sqlContext.implicits._\n",
    "//Convert timestamp to unix timestamp\n",
    "\n",
    "/* ---------------------------------------------------  \n",
    "    Convert timestamp to unix timestamp for Weather\n",
    "    dataframe\n",
    "-------------- -------------------------------------- */\n",
    "val data_w = OT.select(\n",
    "    $\"t0\",\n",
    "    unix_timestamp($\"t0\",\"yyyy-MM-dd HH:mm:ss\").as(\"t0_unix\"),\n",
    "    //(unix_timestamp($\"t0\",\"yyyy-MM-dd HH:mm:ss\")-12*3600).as(\"t0_unix_12\"),\n",
    "    $\"A\",\n",
    "    $\"H\",\n",
    "    $\"Wd\",\n",
    "    $\"Ws\",\n",
    "    $\"P\",\n",
    "    $\"T6\",\n",
    "    $\"S\",\n",
    "    $\"D\",\n",
    "    $\"V\")\n",
    "data_w.printSchema()\n",
    "//data_weather.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17770ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- A: string (nullable = true)\n",
      " |-- t0: string (nullable = true)\n",
      " |-- t0_unix: long (nullable = true)\n",
      " |-- weather: array (nullable = false)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data_weather: org.apache.spark.sql.DataFrame = [A: string, t0: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/* ---------------------------------------------------  \n",
    "    Create a new data frame containing \n",
    "    a weather colummn (array containing time and\n",
    "    weather condition) for Weather dataframe\n",
    "-------------- -------------------------------------- */\n",
    "\n",
    "val data_weather = data_w.select(\n",
    "    $\"A\",\n",
    "    $\"t0\",\n",
    "    $\"t0_unix\",\n",
    "    array($\"t0_unix\",$\"H\",$\"Wd\",$\"Ws\",$\"P\",$\"T6\",$\"S\",$\"D\",$\"V\").as(\"weather\")\n",
    ")\n",
    "data_weather.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6d1ecf",
   "metadata": {},
   "source": [
    "# 4 - Merge Flight dataframe with Weather dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090449b0",
   "metadata": {},
   "source": [
    "## 4.1 - Merge Flight and Weather based on departure airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bc41903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Flights_oA: org.apache.spark.sql.DataFrame = [ORIGIN: string, DEST: string ... 37 more fields]\n",
       "Flights_o1: org.apache.spark.sql.DataFrame = [originK1: string, slot1_D: array<string>]\n",
       "Flights_oA: org.apache.spark.sql.DataFrame = [ORIGIN: string, DEST: string ... 37 more fields]\n",
       "Flights_o2: org.apache.spark.sql.DataFrame = [originK2: string, slot2_D: array<string>]\n",
       "Flights_oA: org.apache.spark.sql.DataFrame = [ORIGIN: string, DEST: string ... 37 more fields]\n",
       "Flights_o3: org.apache.spark.sql.DataFrame = [originK3: string, slot3_D: array<string>]\n",
       "Flights_oA: org.apache.spark.sql.DataFrame = [ORIGIN: string, DEST: string ... 37 more fields]\n",
       "Flights_o4: org.apache.spark.sql.DataFrame = [originK4: string, slot4_D: array<string>]\n",
       "Flights_oA: org.apache.spark.sql.DataFrame = [ORIGIN: string, DES...\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Partie ORIGIN - Departure\n",
    "// les 12 jointures par cr√©neau de 60 minutes et recherche de l'observation la plus proche des -60'\n",
    "// slot1_O √©tant entre Tsd-60' et Tsd, slot2_O entre Tsd-120' et Tsd-60', etc...\n",
    "\n",
    "/* -----------------------------------------------------------\n",
    "    Merge Flight data with Weather data for each time slot\n",
    "    based on flight origin key (originK)\n",
    "    \n",
    "    - step 1 : join flight and weather based on flight origin key\n",
    "               to retrieve all avaible weather data for a time slot\n",
    "               \n",
    "    - step 2 : group the retrieved weather data by key,\n",
    "               sort them and keep only the weather data\n",
    "               closest to the begining of the time slot              \n",
    "-------------- -----------------------------------------------*/\n",
    "\n",
    "var Flights_oA = data_flights.join(data_weather, (data_flights(\"ORIGIN\")===data_weather(\"A\")) && (data_flights(\"Tsd_unix\")>= data_weather(\"t0_unix\")) &&(data_flights(\"Tsd_unix_1\")<= data_weather(\"t0_unix\")) , \"inner\")\n",
    "val Flights_o1 =Flights_oA.groupBy(\"originK\").agg(sort_array(collect_list(\"weather\"),asc = true).getItem(0).alias(\"slot1_D\")).withColumnRenamed(\"originK\",\"originK1\")\n",
    "\n",
    "Flights_oA = data_flights.join(data_weather, (data_flights(\"ORIGIN\")===data_weather(\"A\")) && (data_flights(\"Tsd_unix\")>= data_weather(\"t0_unix\")) &&(data_flights(\"Tsd_unix_2\")<= data_weather(\"t0_unix\")) && (data_flights(\"Tsd_unix_1\")>= data_weather(\"t0_unix\")) , \"inner\")\n",
    "val Flights_o2 =Flights_oA.groupBy(\"originK\").agg(sort_array(collect_list(\"weather\"),true).getItem(0).alias(\"slot2_D\")).withColumnRenamed(\"originK\",\"originK2\")\n",
    "\n",
    "Flights_oA = data_flights.join(data_weather, (data_flights(\"ORIGIN\")===data_weather(\"A\")) && (data_flights(\"Tsd_unix\")>= data_weather(\"t0_unix\")) &&(data_flights(\"Tsd_unix_3\")<= data_weather(\"t0_unix\")) &&(data_flights(\"Tsd_unix_2\")>= data_weather(\"t0_unix\")) , \"inner\")\n",
    "val Flights_o3 =Flights_oA.groupBy(\"originK\").agg(sort_array(collect_list(\"weather\"),true).getItem(0).alias(\"slot3_D\")).withColumnRenamed(\"originK\",\"originK3\")\n",
    "\n",
    "Flights_oA = data_flights.join(data_weather, (data_flights(\"ORIGIN\")===data_weather(\"A\")) && (data_flights(\"Tsd_unix\")>= data_weather(\"t0_unix\")) &&(data_flights(\"Tsd_unix_4\")<= data_weather(\"t0_unix\")) &&(data_flights(\"Tsd_unix_3\")>= data_weather(\"t0_unix\")), \"inner\")\n",
    "val Flights_o4 =Flights_oA.groupBy(\"originK\").agg(sort_array(collect_list(\"weather\"),true).getItem(0).alias(\"slot4_D\")).withColumnRenamed(\"originK\",\"originK4\")\n",
    "\n",
    "Flights_oA = data_flights.join(data_weather, (data_flights(\"ORIGIN\")===data_weather(\"A\")) && (data_flights(\"Tsd_unix\")>= data_weather(\"t0_unix\")) &&(data_flights(\"Tsd_unix_5\")<= data_weather(\"t0_unix\")) &&(data_flights(\"Tsd_unix_4\")>= data_weather(\"t0_unix\")), \"inner\")\n",
    "val Flights_o5 =Flights_oA.groupBy(\"originK\").agg(sort_array(collect_list(\"weather\"),true).getItem(0).alias(\"slot5_D\")).withColumnRenamed(\"originK\",\"originK5\")\n",
    "\n",
    "Flights_oA= data_flights.join(data_weather, (data_flights(\"ORIGIN\")===data_weather(\"A\")) && (data_flights(\"Tsd_unix\")>= data_weather(\"t0_unix\")) &&(data_flights(\"Tsd_unix_6\")<= data_weather(\"t0_unix\")) &&(data_flights(\"Tsd_unix_5\")>= data_weather(\"t0_unix\")), \"inner\")\n",
    "val Flights_o6 =Flights_oA.groupBy(\"originK\").agg(sort_array(collect_list(\"weather\"),true).getItem(0).alias(\"slot6_D\")).withColumnRenamed(\"originK\",\"originK6\")\n",
    "\n",
    "Flights_oA= data_flights.join(data_weather, (data_flights(\"ORIGIN\")===data_weather(\"A\")) && (data_flights(\"Tsd_unix\")>= data_weather(\"t0_unix\")) &&(data_flights(\"Tsd_unix_7\")<= data_weather(\"t0_unix\")) &&(data_flights(\"Tsd_unix_6\")>= data_weather(\"t0_unix\")), \"inner\")\n",
    "val Flights_o7 =Flights_oA.groupBy(\"originK\").agg(sort_array(collect_list(\"weather\"),true).getItem(0).alias(\"slot7_D\")).withColumnRenamed(\"originK\",\"originK7\")\n",
    "\n",
    "Flights_oA= data_flights.join(data_weather, (data_flights(\"ORIGIN\")===data_weather(\"A\")) && (data_flights(\"Tsd_unix\")>= data_weather(\"t0_unix\")) &&(data_flights(\"Tsd_unix_8\")<= data_weather(\"t0_unix\")) &&(data_flights(\"Tsd_unix_7\")>= data_weather(\"t0_unix\")), \"inner\")\n",
    "val Flights_o8 =Flights_oA.groupBy(\"originK\").agg(sort_array(collect_list(\"weather\"),true).getItem(0).alias(\"slot8_D\")).withColumnRenamed(\"originK\",\"originK8\")\n",
    "\n",
    "Flights_oA= data_flights.join(data_weather, (data_flights(\"ORIGIN\")===data_weather(\"A\")) && (data_flights(\"Tsd_unix\")>= data_weather(\"t0_unix\")) &&(data_flights(\"Tsd_unix_9\")<= data_weather(\"t0_unix\")) &&(data_flights(\"Tsd_unix_8\")>= data_weather(\"t0_unix\")), \"inner\")\n",
    "val Flights_o9 =Flights_oA.groupBy(\"originK\").agg(sort_array(collect_list(\"weather\"),true).getItem(0).alias(\"slot9_D\")).withColumnRenamed(\"originK\",\"originK9\")\n",
    "\n",
    "Flights_oA= data_flights.join(data_weather, (data_flights(\"ORIGIN\")===data_weather(\"A\")) && (data_flights(\"Tsd_unix\")>= data_weather(\"t0_unix\")) &&(data_flights(\"Tsd_unix_10\")<= data_weather(\"t0_unix\")) &&(data_flights(\"Tsd_unix_9\")>= data_weather(\"t0_unix\")), \"inner\")\n",
    "val Flights_o10 =Flights_oA.groupBy(\"originK\").agg(sort_array(collect_list(\"weather\"),true).getItem(0).alias(\"slot10_D\")).withColumnRenamed(\"originK\",\"originK10\")\n",
    "\n",
    "Flights_oA= data_flights.join(data_weather, (data_flights(\"ORIGIN\")===data_weather(\"A\")) && (data_flights(\"Tsd_unix\")>= data_weather(\"t0_unix\")) &&(data_flights(\"Tsd_unix_11\")<= data_weather(\"t0_unix\")) &&(data_flights(\"Tsd_unix_10\")>= data_weather(\"t0_unix\")), \"inner\")\n",
    "val Flights_o11 =Flights_oA.groupBy(\"originK\").agg(sort_array(collect_list(\"weather\"),true).getItem(0).alias(\"slot11_D\")).withColumnRenamed(\"originK\",\"originK11\")\n",
    "\n",
    "Flights_oA= data_flights.join(data_weather, (data_flights(\"ORIGIN\")===data_weather(\"A\")) && (data_flights(\"Tsd_unix\")>= data_weather(\"t0_unix\")) &&(data_flights(\"Tsd_unix_12\")<= data_weather(\"t0_unix\")) &&(data_flights(\"Tsd_unix_11\")>= data_weather(\"t0_unix\")), \"inner\")\n",
    "val Flights_o12 =Flights_oA.groupBy(\"originK\").agg(sort_array(collect_list(\"weather\"),true).getItem(0).alias(\"slot12_D\")).withColumnRenamed(\"originK\",\"originK12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51ce2ee",
   "metadata": {},
   "source": [
    "## 4.2 - Merge Flight and Weather based on arrival airport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9707def0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Flights_dA: org.apache.spark.sql.DataFrame = [ORIGIN: string, DEST: string ... 37 more fields]\n",
       "Flights_d1: org.apache.spark.sql.DataFrame = [destK1: string, slot1_A: array<string>]\n",
       "Flights_dA: org.apache.spark.sql.DataFrame = [ORIGIN: string, DEST: string ... 37 more fields]\n",
       "Flights_d2: org.apache.spark.sql.DataFrame = [destK2: string, slot2_A: array<string>]\n",
       "Flights_dA: org.apache.spark.sql.DataFrame = [ORIGIN: string, DEST: string ... 37 more fields]\n",
       "Flights_d3: org.apache.spark.sql.DataFrame = [destK3: string, slot3_A: array<string>]\n",
       "Flights_dA: org.apache.spark.sql.DataFrame = [ORIGIN: string, DEST: string ... 37 more fields]\n",
       "Flights_d4: org.apache.spark.sql.DataFrame = [destK4: string, slot4_A: array<string>]\n",
       "Flights_dA: org.apache.spark.sql.DataFrame = [ORIGIN: string, DEST: strin...\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Partie DEST - Arrival\n",
    "// les 12 jointures par cr√©neau de 60 minutes et recherche de l'observation la plus proche des -60'\n",
    "// slot1_D √©tant entre Tsa-60' et Tsa, slot2_D entre Tsa-120' et Tad-60', etc...\n",
    "\n",
    "/* -----------------------------------------------------------\n",
    "    Merge Flight data with Weather data for each time slot\n",
    "    based on flight arrival key (destK)\n",
    "    \n",
    "    - step 1 : join flight and weather based on flight origin key\n",
    "               to retrieve all avaible weather data for a time slot\n",
    "               \n",
    "    - step 2 : group the retrieved weather data by key,\n",
    "               sort them and keep only the weather data\n",
    "               closest to the begining of the time slot              \n",
    "-------------- -----------------------------------------------*/\n",
    "\n",
    "var Flights_dA = data_flights.join(data_weather, (data_flights(\"DEST\")===data_weather(\"A\")) && (data_flights(\"Tsa_unix\")>= data_weather(\"t0_unix\")) &&(data_flights(\"Tsa_unix_1\")<= data_weather(\"t0_unix\")) , \"inner\")\n",
    "val Flights_d1 =Flights_dA.groupBy(\"destK\").agg(sort_array(collect_list(\"weather\"),asc = true).getItem(0).alias(\"slot1_A\")).withColumnRenamed(\"destK\",\"destK1\")\n",
    "\n",
    "Flights_dA = data_flights.join(data_weather, (data_flights(\"DEST\")===data_weather(\"A\")) && (data_flights(\"Tsa_unix\")>= data_weather(\"t0_unix\")) &&(data_flights(\"Tsa_unix_2\")<= data_weather(\"t0_unix\")) && (data_flights(\"Tsa_unix_1\")>= data_weather(\"t0_unix\")) , \"inner\")\n",
    "val Flights_d2 =Flights_dA.groupBy(\"destK\").agg(sort_array(collect_list(\"weather\"),true).getItem(0).alias(\"slot2_A\")).withColumnRenamed(\"destK\",\"destK2\")\n",
    "\n",
    "Flights_dA = data_flights.join(data_weather, (data_flights(\"DEST\")===data_weather(\"A\")) && (data_flights(\"Tsa_unix\")>= data_weather(\"t0_unix\")) &&(data_flights(\"Tsa_unix_3\")<= data_weather(\"t0_unix\")) &&(data_flights(\"Tsa_unix_2\")>= data_weather(\"t0_unix\")) , \"inner\")\n",
    "val Flights_d3 =Flights_dA.groupBy(\"destK\").agg(sort_array(collect_list(\"weather\"),true).getItem(0).alias(\"slot3_A\")).withColumnRenamed(\"destK\",\"destK3\")\n",
    "\n",
    "Flights_dA = data_flights.join(data_weather, (data_flights(\"DEST\")===data_weather(\"A\")) && (data_flights(\"Tsa_unix\")>= data_weather(\"t0_unix\")) &&(data_flights(\"Tsa_unix_4\")<= data_weather(\"t0_unix\")) &&(data_flights(\"Tsa_unix_3\")>= data_weather(\"t0_unix\")), \"inner\")\n",
    "val Flights_d4 =Flights_dA.groupBy(\"destK\").agg(sort_array(collect_list(\"weather\"),true).getItem(0).alias(\"slot4_A\")).withColumnRenamed(\"destK\",\"destK4\")\n",
    "\n",
    "Flights_dA = data_flights.join(data_weather, (data_flights(\"DEST\")===data_weather(\"A\")) && (data_flights(\"Tsa_unix\")>= data_weather(\"t0_unix\")) &&(data_flights(\"Tsa_unix_5\")<= data_weather(\"t0_unix\")) &&(data_flights(\"Tsa_unix_4\")>= data_weather(\"t0_unix\")), \"inner\")\n",
    "val Flights_d5 =Flights_dA.groupBy(\"destK\").agg(sort_array(collect_list(\"weather\"),true).getItem(0).alias(\"slot5_A\")).withColumnRenamed(\"destK\",\"destK5\")\n",
    "\n",
    "Flights_dA= data_flights.join(data_weather, (data_flights(\"DEST\")===data_weather(\"A\")) && (data_flights(\"Tsa_unix\")>= data_weather(\"t0_unix\")) &&(data_flights(\"Tsa_unix_6\")<= data_weather(\"t0_unix\")) &&(data_flights(\"Tsa_unix_5\")>= data_weather(\"t0_unix\")), \"inner\")\n",
    "val Flights_d6 =Flights_dA.groupBy(\"destK\").agg(sort_array(collect_list(\"weather\"),true).getItem(0).alias(\"slot6_A\")).withColumnRenamed(\"destK\",\"destK6\")\n",
    "\n",
    "Flights_dA= data_flights.join(data_weather, (data_flights(\"DEST\")===data_weather(\"A\")) && (data_flights(\"Tsa_unix\")>= data_weather(\"t0_unix\")) &&(data_flights(\"Tsa_unix_7\")<= data_weather(\"t0_unix\")) &&(data_flights(\"Tsa_unix_6\")>= data_weather(\"t0_unix\")), \"inner\")\n",
    "val Flights_d7 =Flights_dA.groupBy(\"destK\").agg(sort_array(collect_list(\"weather\"),true).getItem(0).alias(\"slot7_A\")).withColumnRenamed(\"destK\",\"destK7\")\n",
    "\n",
    "Flights_dA= data_flights.join(data_weather, (data_flights(\"DEST\")===data_weather(\"A\")) && (data_flights(\"Tsa_unix\")>= data_weather(\"t0_unix\")) &&(data_flights(\"Tsa_unix_8\")<= data_weather(\"t0_unix\")) &&(data_flights(\"Tsa_unix_7\")>= data_weather(\"t0_unix\")), \"inner\")\n",
    "val Flights_d8 =Flights_dA.groupBy(\"destK\").agg(sort_array(collect_list(\"weather\"),true).getItem(0).alias(\"slot8_A\")).withColumnRenamed(\"destK\",\"destK8\")\n",
    "\n",
    "Flights_dA= data_flights.join(data_weather, (data_flights(\"DEST\")===data_weather(\"A\")) && (data_flights(\"Tsa_unix\")>= data_weather(\"t0_unix\")) &&(data_flights(\"Tsa_unix_9\")<= data_weather(\"t0_unix\")) &&(data_flights(\"Tsa_unix_8\")>= data_weather(\"t0_unix\")), \"inner\")\n",
    "val Flights_d9 =Flights_dA.groupBy(\"destK\").agg(sort_array(collect_list(\"weather\"),true).getItem(0).alias(\"slot9_A\")).withColumnRenamed(\"destK\",\"destK9\")\n",
    "\n",
    "Flights_dA= data_flights.join(data_weather, (data_flights(\"DEST\")===data_weather(\"A\")) && (data_flights(\"Tsa_unix\")>= data_weather(\"t0_unix\")) &&(data_flights(\"Tsa_unix_10\")<= data_weather(\"t0_unix\")) &&(data_flights(\"Tsa_unix_9\")>= data_weather(\"t0_unix\")), \"inner\")\n",
    "val Flights_d10 =Flights_dA.groupBy(\"destK\").agg(sort_array(collect_list(\"weather\"),true).getItem(0).alias(\"slot10_A\")).withColumnRenamed(\"destK\",\"destK10\")\n",
    "\n",
    "Flights_dA= data_flights.join(data_weather, (data_flights(\"DEST\")===data_weather(\"A\")) && (data_flights(\"Tsa_unix\")>= data_weather(\"t0_unix\")) &&(data_flights(\"Tsa_unix_11\")<= data_weather(\"t0_unix\")) &&(data_flights(\"Tsa_unix_10\")>= data_weather(\"t0_unix\")), \"inner\")\n",
    "val Flights_d11 =Flights_dA.groupBy(\"destK\").agg(sort_array(collect_list(\"weather\"),true).getItem(0).alias(\"slot11_A\")).withColumnRenamed(\"destK\",\"destK11\")\n",
    "\n",
    "Flights_dA= data_flights.join(data_weather, (data_flights(\"DEST\")===data_weather(\"A\")) && (data_flights(\"Tsa_unix\")>= data_weather(\"t0_unix\")) &&(data_flights(\"Tsa_unix_12\")<= data_weather(\"t0_unix\")) &&(data_flights(\"Tsa_unix_11\")>= data_weather(\"t0_unix\")), \"inner\")\n",
    "val Flights_d12 =Flights_dA.groupBy(\"destK\").agg(sort_array(collect_list(\"weather\"),true).getItem(0).alias(\"slot12_A\")).withColumnRenamed(\"destK\",\"destK12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f1d4c2",
   "metadata": {},
   "source": [
    "## 4.3 - Final join (sequencial joins of the previous 24 joint) to get one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31cdb0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- originK: string (nullable = true)\n",
      " |-- destK: string (nullable = true)\n",
      " |-- Tad_unix: long (nullable = true)\n",
      " |-- Tad_unix_12: long (nullable = true)\n",
      " |-- Tsd_unix: long (nullable = true)\n",
      " |-- Tsd_unix_1: long (nullable = true)\n",
      " |-- Tsd_unix_2: long (nullable = true)\n",
      " |-- Tsd_unix_3: long (nullable = true)\n",
      " |-- Tsd_unix_4: long (nullable = true)\n",
      " |-- Tsd_unix_5: long (nullable = true)\n",
      " |-- Tsd_unix_6: long (nullable = true)\n",
      " |-- Tsd_unix_7: long (nullable = true)\n",
      " |-- Tsd_unix_8: long (nullable = true)\n",
      " |-- Tsd_unix_9: long (nullable = true)\n",
      " |-- Tsd_unix_10: long (nullable = true)\n",
      " |-- Tsd_unix_11: long (nullable = true)\n",
      " |-- Tsd_unix_12: long (nullable = true)\n",
      " |-- Taa_unix: long (nullable = true)\n",
      " |-- Taa_unix_12: long (nullable = true)\n",
      " |-- Tsa_unix: long (nullable = true)\n",
      " |-- Tsa_unix_1: long (nullable = true)\n",
      " |-- Tsa_unix_2: long (nullable = true)\n",
      " |-- Tsa_unix_3: long (nullable = true)\n",
      " |-- Tsa_unix_4: long (nullable = true)\n",
      " |-- Tsa_unix_5: long (nullable = true)\n",
      " |-- Tsa_unix_6: long (nullable = true)\n",
      " |-- Tsa_unix_7: long (nullable = true)\n",
      " |-- Tsa_unix_8: long (nullable = true)\n",
      " |-- Tsa_unix_9: long (nullable = true)\n",
      " |-- Tsa_unix_10: long (nullable = true)\n",
      " |-- Tsa_unix_11: long (nullable = true)\n",
      " |-- Tsa_unix_12: long (nullable = true)\n",
      " |-- Th: boolean (nullable = true)\n",
      " |-- originK1: string (nullable = true)\n",
      " |-- slot1_D: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- originK2: string (nullable = true)\n",
      " |-- slot2_D: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- originK3: string (nullable = true)\n",
      " |-- slot3_D: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- originK4: string (nullable = true)\n",
      " |-- slot4_D: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- originK5: string (nullable = true)\n",
      " |-- slot5_D: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- originK6: string (nullable = true)\n",
      " |-- slot6_D: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- originK7: string (nullable = true)\n",
      " |-- slot7_D: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- originK8: string (nullable = true)\n",
      " |-- slot8_D: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- originK9: string (nullable = true)\n",
      " |-- slot9_D: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- originK10: string (nullable = true)\n",
      " |-- slot10_D: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- originK11: string (nullable = true)\n",
      " |-- slot11_D: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- originK12: string (nullable = true)\n",
      " |-- slot12_D: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- slot1_A: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- slot2_A: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- slot3_A: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- slot4_A: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- slot5_A: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- slot6_A: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- slot7_A: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- slot8_A: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- slot9_A: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- slot10_A: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- slot11_A: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- slot12_A: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data_flights_part1: org.apache.spark.sql.DataFrame = [ORIGIN: string, DEST: string ... 69 more fields]\n",
       "data_final: data_flights_part1.type = [ORIGIN: string, DEST: string ... 69 more fields]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/* -----------------------------------------------------------\n",
    "        Proceed the 24 joins to get the final data frame\n",
    "        containing flights and weather data for each \n",
    "        flight origin, destination and time slot\n",
    "-------------- -----------------------------------------------*/\n",
    "\n",
    "val data_flights_part1 = data_flights\n",
    "\n",
    "                         // les 12 jointures finales ORIGIN - Departure\n",
    "                        .join(Flights_o1,data_flights(\"originK\")===Flights_o1(\"originK1\"),\"left_outer\")\n",
    "                        .join(Flights_o2,data_flights(\"originK\")===Flights_o2(\"originK2\"),\"left_outer\")\n",
    "                        .join(Flights_o3,data_flights(\"originK\")===Flights_o3(\"originK3\"),\"left_outer\")\n",
    "                        .join(Flights_o4,data_flights(\"originK\")===Flights_o4(\"originK4\"),\"left_outer\")\n",
    "                        .join(Flights_o5,data_flights(\"originK\")===Flights_o5(\"originK5\"),\"left_outer\")\n",
    "                        .join(Flights_o6,data_flights(\"originK\")===Flights_o6(\"originK6\"),\"left_outer\")\n",
    "                        .join(Flights_o7,data_flights(\"originK\")===Flights_o7(\"originK7\"),\"left_outer\")\n",
    "                        .join(Flights_o8,data_flights(\"originK\")===Flights_o8(\"originK8\"),\"left_outer\")\n",
    "                        .join(Flights_o9,data_flights(\"originK\")===Flights_o9(\"originK9\"),\"left_outer\")\n",
    "                        .join(Flights_o10,data_flights(\"originK\")===Flights_o10(\"originK10\"),\"left_outer\")\n",
    "                        .join(Flights_o11,data_flights(\"originK\")===Flights_o11(\"originK11\"),\"left_outer\")\n",
    "                        .join(Flights_o12,data_flights(\"originK\")===Flights_o12(\"originK12\"),\"left_outer\")\n",
    "\n",
    "                         // les 12 jointures finales DESTINATION\n",
    "                        .join(Flights_d1,data_flights(\"destK\")===Flights_d1(\"destK1\"),\"left_outer\")\n",
    "                        .join(Flights_d2,data_flights(\"destK\")===Flights_d2(\"destK2\"),\"left_outer\")\n",
    "                        .join(Flights_d3,data_flights(\"destK\")===Flights_d3(\"destK3\"),\"left_outer\")\n",
    "                        .join(Flights_d4,data_flights(\"destK\")===Flights_d4(\"destK4\"),\"left_outer\")\n",
    "                        .join(Flights_d5,data_flights(\"destK\")===Flights_d5(\"destK5\"),\"left_outer\")\n",
    "                        .join(Flights_d6,data_flights(\"destK\")===Flights_d6(\"destK6\"),\"left_outer\")\n",
    "                        .join(Flights_d7,data_flights(\"destK\")===Flights_d7(\"destK7\"),\"left_outer\")\n",
    "                        .join(Flights_d8,data_flights(\"destK\")===Flights_d8(\"destK8\"),\"left_outer\")\n",
    "                        .join(Flights_d9,data_flights(\"destK\")===Flights_d9(\"destK9\"),\"left_outer\")\n",
    "                        .join(Flights_d10,data_flights(\"destK\")===Flights_d10(\"destK10\"),\"left_outer\")\n",
    "                        .join(Flights_d11,data_flights(\"destK\")===Flights_d11(\"destK11\"),\"left_outer\")\n",
    "                        .join(Flights_d12,data_flights(\"destK\")===Flights_d12(\"destK12\"),\"left_outer\")\n",
    "\n",
    "                         // suppression des champs interm√©diaires d√©sormais inutiles\n",
    "                        //.drop(\"originK1\",\"originK2\",\"originK3\",\"originK4\",\"originK5\",\"originK6\",\"originK7\",\"originK8\",\"originK9\",\"originK10\",\"originK11\",\"originK12\")\n",
    "                        .drop(\"destK1\",\"destK2\",\"destK3\",\"destK4\",\"destK5\",\"destK6\",\"destK7\",\"destK8\",\"destK9\",\"destK10\",\"destK11\",\"destK12\")\n",
    "\n",
    "val data_final = data_flights_part1.cache()\n",
    "\n",
    "// Dur√©e d'environ 4'30 minutes sur Mac sans optimisation coalesce, persist etc...\n",
    "\n",
    "data_final.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eae6cd",
   "metadata": {},
   "source": [
    "## 4.4 - Save the final data as a parquet file to be used in ML pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd504999",
   "metadata": {},
   "outputs": [],
   "source": [
    "/* -----------------------------------------------------------\n",
    "            Save the final data as a parquet file\n",
    "            in a new folder called data_parquet\n",
    "-------------- -----------------------------------------------*/\n",
    "! mkdir data_parquet\n",
    "data_final.coalesce(4).write.parquet(\"data_parquet/flight_1M.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bde0d73",
   "metadata": {},
   "source": [
    "## 4.5 - Load the parquet to ensure that it has been generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7effb1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- originK: string (nullable = true)\n",
      " |-- destK: string (nullable = true)\n",
      " |-- Tad_unix: long (nullable = true)\n",
      " |-- Tad_unix_12: long (nullable = true)\n",
      " |-- Tsd_unix: long (nullable = true)\n",
      " |-- Tsd_unix_1: long (nullable = true)\n",
      " |-- Tsd_unix_2: long (nullable = true)\n",
      " |-- Tsd_unix_3: long (nullable = true)\n",
      " |-- Tsd_unix_4: long (nullable = true)\n",
      " |-- Tsd_unix_5: long (nullable = true)\n",
      " |-- Tsd_unix_6: long (nullable = true)\n",
      " |-- Tsd_unix_7: long (nullable = true)\n",
      " |-- Tsd_unix_8: long (nullable = true)\n",
      " |-- Tsd_unix_9: long (nullable = true)\n",
      " |-- Tsd_unix_10: long (nullable = true)\n",
      " |-- Tsd_unix_11: long (nullable = true)\n",
      " |-- Tsd_unix_12: long (nullable = true)\n",
      " |-- Taa_unix: long (nullable = true)\n",
      " |-- Taa_unix_12: long (nullable = true)\n",
      " |-- Tsa_unix: long (nullable = true)\n",
      " |-- Tsa_unix_1: long (nullable = true)\n",
      " |-- Tsa_unix_2: long (nullable = true)\n",
      " |-- Tsa_unix_3: long (nullable = true)\n",
      " |-- Tsa_unix_4: long (nullable = true)\n",
      " |-- Tsa_unix_5: long (nullable = true)\n",
      " |-- Tsa_unix_6: long (nullable = true)\n",
      " |-- Tsa_unix_7: long (nullable = true)\n",
      " |-- Tsa_unix_8: long (nullable = true)\n",
      " |-- Tsa_unix_9: long (nullable = true)\n",
      " |-- Tsa_unix_10: long (nullable = true)\n",
      " |-- Tsa_unix_11: long (nullable = true)\n",
      " |-- Tsa_unix_12: long (nullable = true)\n",
      " |-- Th: boolean (nullable = true)\n",
      " |-- originK1: string (nullable = true)\n",
      " |-- slot1_D: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- originK2: string (nullable = true)\n",
      " |-- slot2_D: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- originK3: string (nullable = true)\n",
      " |-- slot3_D: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- originK4: string (nullable = true)\n",
      " |-- slot4_D: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- originK5: string (nullable = true)\n",
      " |-- slot5_D: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- originK6: string (nullable = true)\n",
      " |-- slot6_D: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- originK7: string (nullable = true)\n",
      " |-- slot7_D: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- originK8: string (nullable = true)\n",
      " |-- slot8_D: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- originK9: string (nullable = true)\n",
      " |-- slot9_D: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- originK10: string (nullable = true)\n",
      " |-- slot10_D: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- originK11: string (nullable = true)\n",
      " |-- slot11_D: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- originK12: string (nullable = true)\n",
      " |-- slot12_D: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- slot1_A: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- slot2_A: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- slot3_A: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- slot4_A: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- slot5_A: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- slot6_A: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- slot7_A: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- slot8_A: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- slot9_A: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- slot10_A: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- slot11_A: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- slot12_A: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+------+----+-------------+-------------+----------+-----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+-----------+-----------+-----------+----------+-----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+-----------+-----------+-----------+-----+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|ORIGIN|DEST|      originK|        destK|  Tad_unix|Tad_unix_12|  Tsd_unix|Tsd_unix_1|Tsd_unix_2|Tsd_unix_3|Tsd_unix_4|Tsd_unix_5|Tsd_unix_6|Tsd_unix_7|Tsd_unix_8|Tsd_unix_9|Tsd_unix_10|Tsd_unix_11|Tsd_unix_12|  Taa_unix|Taa_unix_12|  Tsa_unix|Tsa_unix_1|Tsa_unix_2|Tsa_unix_3|Tsa_unix_4|Tsa_unix_5|Tsa_unix_6|Tsa_unix_7|Tsa_unix_8|Tsa_unix_9|Tsa_unix_10|Tsa_unix_11|Tsa_unix_12|   Th|     originK1|             slot1_D|     originK2|             slot2_D|     originK3|             slot3_D|     originK4|             slot4_D|     originK5|             slot5_D|     originK6|             slot6_D|     originK7|             slot7_D|     originK8|             slot8_D|     originK9|             slot9_D|    originK10|            slot10_D|    originK11|            slot11_D|    originK12|            slot12_D|             slot1_A|             slot2_A|             slot3_A|             slot4_A|             slot5_A|             slot6_A|             slot7_A|             slot8_A|             slot9_A|            slot10_A|            slot11_A|            slot12_A|\n",
      "+------+----+-------------+-------------+----------+-----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+-----------+-----------+-----------+----------+-----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+-----------+-----------+-----------+-----+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|   OAK| BUR|OAK1327693200|BUR1327697100|1327693200| 1327650000|1327693200|1327689600|1327686000|1327682400|1327678800|1327675200|1327671600|1327668000|1327664400|1327660800| 1327657200| 1327653600| 1327650000|1327696680| 1327653480|1327697100|1327693500|1327689900|1327686300|1327682700|1327679100|1327675500|1327671900|1327668300|1327664700| 1327661100| 1327657500| 1327653900|false|OAK1327693200|[1327690380, 59.0...|OAK1327693200|[1327686780, 43.0...|OAK1327693200|[1327683180, 48.0...|OAK1327693200|[1327679580, 39.0...|OAK1327693200|[1327675980, 35.0...|OAK1327693200|[1327672380, 46.0...|OAK1327693200|[1327668780, 57.0...|OAK1327693200|[1327665180, 83.0...|OAK1327693200|[1327660860, 100....|OAK1327693200|[1327657980, 100....|OAK1327693200|[1327654380, 100....|OAK1327693200|[1327650660, 100....|[1327693980, 15.0...|[1327690380, 17.0...|[1327686780, 18.0...|[1327683180, 16.0...|[1327679580, 16.0...|[1327675980, 16.0...|[1327672380, 15.0...|[1327668780, 18.0...|[1327665180, 34.0...|[1327661580, 50.0...|[1327657980, 54.0...|[1327654380, 62.0...|\n",
      "|   ORD| CMH|ORD1326412320|CMH1326328500|1326412320| 1326369120|1326407400|1326403800|1326400200|1326396600|1326393000|1326389400|1326385800|1326382200|1326378600|1326375000| 1326371400| 1326367800| 1326364200|1326336480| 1326293280|1326328500|1326324900|1326321300|1326317700|1326314100|1326310500|1326306900|1326303300|1326299700|1326296100| 1326292500| 1326288900| 1326285300| true|ORD1326412320|[1326405060, 84.0...|ORD1326412320|[1326401340, 88.0...|ORD1326412320|[1326396660, 85.0...|ORD1326412320|[1326394140, 78.0...|ORD1326412320|[1326390660, 81.0...|ORD1326412320|[1326386100, 85.0...|ORD1326412320|[1326383460, 88.0...|ORD1326412320|[1326379860, 88.0...|ORD1326412320|[1326376260, 85.0...|ORD1326412320|[1326372660, 89.0...|ORD1326412320|[1326369060, 89.0...|ORD1326412320|[1326365460, 89.0...|[1326325860, 93.0...|[1326322260, 93.0...|[1326318660, 96.0...|[1326315060, 96.0...|[1326310980, 100....|[1326307860, 96.0...|[1326304260, 93.0...|[1326300660, 93.0...|[1326297060, 96.0...|[1326293460, 93.0...|[1326289860, 96.0...|[1326286260, 93.0...|\n",
      "|   ORD| CMH|ORD1327257000|CMH1327263300|1327257000| 1327213800|1327255200|1327251600|1327248000|1327244400|1327240800|1327237200|1327233600|1327230000|1327226400|1327222800| 1327219200| 1327215600| 1327212000|1327266360| 1327223160|1327263300|1327259700|1327256100|1327252500|1327248900|1327245300|1327241700|1327238100|1327234500|1327230900| 1327227300| 1327223700| 1327220100| true|ORD1327257000|[1327252320, 100....|ORD1327257000|[1327250820, 92.0...|ORD1327257000|[1327246020, 85.0...|ORD1327257000|[1327243860, 92.0...|ORD1327257000|[1327240140, 92.0...|ORD1327257000|[1327236660, 89.0...|ORD1327257000|[1327233060, 92.0...|ORD1327257000|[1327229460, 92.0...|ORD1327257000|[1327225860, 88.0...|ORD1327257000|[1327221780, 92.0...|ORD1327257000|[1327216860, 92.0...|ORD1327257000|[1327215060, 88.0...|[1327261860, 96.0...|[1327256880, 96.0...|[1327254660, 100....|[1327249380, 92.0...|[1327247040, 92.0...|[1327243860, 92.0...|[1327240260, 96.0...|[1327234800, 85.0...|[1327231320, 92.0...|[1327229460, 96.0...|[1327225860, 96.0...|[1327222260, 96.0...|\n",
      "|   TUL| DAL|TUL1326261420|DAL1326265500|1326261420| 1326218220|1326261600|1326258000|1326254400|1326250800|1326247200|1326243600|1326240000|1326236400|1326232800|1326229200| 1326225600| 1326222000| 1326218400|1326265380| 1326222180|1326265500|1326261900|1326258300|1326254700|1326251100|1326247500|1326243900|1326240300|1326236700|1326233100| 1326229500| 1326225900| 1326222300|false|TUL1326261420|[1326261180, 92.0...|TUL1326261420|[1326257580, 96.0...|TUL1326261420|[1326253980, 89.0...|TUL1326261420|[1326250380, 92.0...|TUL1326261420|[1326246780, 89.0...|TUL1326261420|[1326243180, 79.0...|TUL1326261420|[1326239580, 79.0...|TUL1326261420|[1326235980, 71.0...|TUL1326261420|[1326232380, 52.0...|TUL1326261420|[1326228780, 56.0...|TUL1326261420|[1326225180, 58.0...|TUL1326261420|[1326221580, 46.0...|[1326264780, 86.0...|[1326261180, 86.0...|[1326257580, 89.0...|[1326253980, 89.0...|[1326250380, 93.0...|[1326246780, 86.0...|[1326243180, 86.0...|[1326239580, 83.0...|[1326235980, 77.0...|[1326232380, 83.0...|[1326228780, 86.0...|[1326225180, 83.0...|\n",
      "|   GEG| DEN|GEG1326815520|DEN1326825900|1326815520| 1326772320|1326814500|1326810900|1326807300|1326803700|1326800100|1326796500|1326792900|1326789300|1326785700|1326782100| 1326778500| 1326774900| 1326771300|1326827280| 1326784080|1326825900|1326822300|1326818700|1326815100|1326811500|1326807900|1326804300|1326800700|1326797100|1326793500| 1326789900| 1326786300| 1326782700| true|GEG1326815520|[1326811860, 70.0...|GEG1326815520|[1326808380, 70.0...|GEG1326815520|[1326804780, 67.0...|GEG1326815520|[1326801180, 61.0...|GEG1326815520|[1326797460, 56.0...|GEG1326815520|[1326793980, 59.0...|GEG1326815520|[1326790380, 58.0...|GEG1326815520|[1326786780, 58.0...|GEG1326815520|[1326783180, 58.0...|GEG1326815520|[1326779580, 58.0...|GEG1326815520|[1326775980, 58.0...|GEG1326815520|[1326772380, 61.0...|[1326822780, 68.0...|[1326819180, 57.0...|[1326815580, 43.0...|[1326811980, 37.0...|[1326808380, 43.0...|[1326804780, 55.0...|[1326801180, 50.0...|[1326797580, 45.0...|[1326793980, 59.0...|[1326790380, 67.0...|[1326786780, 69.0...|[1326783180, 69.0...|\n",
      "+------+----+-------------+-------------+----------+-----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+-----------+-----------+-----------+----------+-----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+-----------+-----------+-----------+-----+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "JT_parquet: org.apache.spark.sql.DataFrame = [ORIGIN: string, DEST: string ... 69 more fields]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/* -----------------------------------------------------------\n",
    "    load the parquet to ensure that it has been generated\n",
    "-------------- -----------------------------------------------*/\n",
    "val JT_parquet  = spark.read.parquet(\"data_parquet/flight_1M.parquet\")\n",
    "JT_parquet.printSchema()\n",
    "JT_parquet.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde0fad9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
