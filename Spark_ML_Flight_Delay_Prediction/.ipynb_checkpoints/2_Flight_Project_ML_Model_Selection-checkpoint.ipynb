{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4645b08",
   "metadata": {},
   "source": [
    "# 0 - Load the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc5b5c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://0d06e632b0ca:4042\n",
       "SparkContext available as 'sc' (version = 3.1.1, master = local[*], app id = local-1621270407163)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.sql.types.DoubleType\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.functions.rand\n",
       "import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel, Tokenizer}\n",
       "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer, VectorAssembler}\n",
       "import org.apache.spark.ml.feature.OneHotEncoder\n",
       "import org.apache.spark.ml.Pipeline\n",
       "import org.apache.spark.ml.PipelineModel\n",
       "import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier, LogisticRegression, LogisticRegressionModel, GBTClassificationModel, GBTClassifier}\n",
       "import org.apache.spark.ml.evaluation.{MulticlassClassificationEvaluator, BinaryClassificationEvaluator}\n",
       "import org.apache.spark.ml.tu...\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types.DoubleType\n",
    "import org.apache.spark.sql.functions._ // ajout hbc 9 oct 2020, nécessaire pour le split et col\n",
    "import org.apache.spark.sql.functions.rand\n",
    "\n",
    "\n",
    "/* ---------------------------------------------------  \n",
    "    Import Spark ML modules\n",
    "-------------- -------------------------------------- */\n",
    "import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel, Tokenizer}\n",
    "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer, VectorAssembler} //, OneHotEncoderEstimator}\n",
    "\n",
    "import org.apache.spark.ml.feature.OneHotEncoder // hbc le 9 oct 2020 : OneHotEncoderEstimator est une version trop récente\n",
    "\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.PipelineModel\n",
    "\n",
    "import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier, LogisticRegression, LogisticRegressionModel, GBTClassificationModel, GBTClassifier}\n",
    "\n",
    "import org.apache.spark.ml.evaluation.{MulticlassClassificationEvaluator, BinaryClassificationEvaluator}\n",
    "\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, TrainValidationSplit, ParamGridBuilder,CrossValidatorModel}\n",
    "\n",
    "import org.apache.spark.ml.param.ParamMap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64825e64",
   "metadata": {},
   "source": [
    "# 1 - Start a Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6b4538",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "val spark = SparkSession\n",
    ".builder\n",
    ".appName(\"flight\")\n",
    ".config(\"spark.master\", \"local\")\n",
    ".getOrCreate()\n",
    "\n",
    "import spark.implicits._\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e7e83d",
   "metadata": {},
   "source": [
    "# 3 - Load the data from parquet file and preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d47c8a",
   "metadata": {},
   "source": [
    "## 3.1 - Load the data from parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3f1eaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-------------+-------------+----------+-----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+-----------+-----------+-----------+----------+-----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+-----------+-----------+-----------+-----+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|ORIGIN|DEST|      originK|        destK|  Tad_unix|Tad_unix_12|  Tsd_unix|Tsd_unix_1|Tsd_unix_2|Tsd_unix_3|Tsd_unix_4|Tsd_unix_5|Tsd_unix_6|Tsd_unix_7|Tsd_unix_8|Tsd_unix_9|Tsd_unix_10|Tsd_unix_11|Tsd_unix_12|  Taa_unix|Taa_unix_12|  Tsa_unix|Tsa_unix_1|Tsa_unix_2|Tsa_unix_3|Tsa_unix_4|Tsa_unix_5|Tsa_unix_6|Tsa_unix_7|Tsa_unix_8|Tsa_unix_9|Tsa_unix_10|Tsa_unix_11|Tsa_unix_12|   Th|     originK1|             slot1_D|     originK2|             slot2_D|     originK3|             slot3_D|     originK4|             slot4_D|     originK5|             slot5_D|     originK6|             slot6_D|     originK7|             slot7_D|     originK8|             slot8_D|     originK9|             slot9_D|    originK10|            slot10_D|    originK11|            slot11_D|    originK12|            slot12_D|             slot1_A|             slot2_A|             slot3_A|             slot4_A|             slot5_A|             slot6_A|             slot7_A|             slot8_A|             slot9_A|            slot10_A|            slot11_A|            slot12_A|\n",
      "+------+----+-------------+-------------+----------+-----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+-----------+-----------+-----------+----------+-----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+-----------+-----------+-----------+-----+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|   OAK| BUR|OAK1327693200|BUR1327697100|1327693200| 1327650000|1327693200|1327689600|1327686000|1327682400|1327678800|1327675200|1327671600|1327668000|1327664400|1327660800| 1327657200| 1327653600| 1327650000|1327696680| 1327653480|1327697100|1327693500|1327689900|1327686300|1327682700|1327679100|1327675500|1327671900|1327668300|1327664700| 1327661100| 1327657500| 1327653900|false|OAK1327693200|[1327690380, 59.0...|OAK1327693200|[1327686780, 43.0...|OAK1327693200|[1327683180, 48.0...|OAK1327693200|[1327679580, 39.0...|OAK1327693200|[1327675980, 35.0...|OAK1327693200|[1327672380, 46.0...|OAK1327693200|[1327668780, 57.0...|OAK1327693200|[1327665180, 83.0...|OAK1327693200|[1327660860, 100....|OAK1327693200|[1327657980, 100....|OAK1327693200|[1327654380, 100....|OAK1327693200|[1327650660, 100....|[1327693980, 15.0...|[1327690380, 17.0...|[1327686780, 18.0...|[1327683180, 16.0...|[1327679580, 16.0...|[1327675980, 16.0...|[1327672380, 15.0...|[1327668780, 18.0...|[1327665180, 34.0...|[1327661580, 50.0...|[1327657980, 54.0...|[1327654380, 62.0...|\n",
      "|   ORD| CMH|ORD1326412320|CMH1326328500|1326412320| 1326369120|1326407400|1326403800|1326400200|1326396600|1326393000|1326389400|1326385800|1326382200|1326378600|1326375000| 1326371400| 1326367800| 1326364200|1326336480| 1326293280|1326328500|1326324900|1326321300|1326317700|1326314100|1326310500|1326306900|1326303300|1326299700|1326296100| 1326292500| 1326288900| 1326285300| true|ORD1326412320|[1326405060, 84.0...|ORD1326412320|[1326401340, 88.0...|ORD1326412320|[1326396660, 85.0...|ORD1326412320|[1326394140, 78.0...|ORD1326412320|[1326390660, 81.0...|ORD1326412320|[1326386100, 85.0...|ORD1326412320|[1326383460, 88.0...|ORD1326412320|[1326379860, 88.0...|ORD1326412320|[1326376260, 85.0...|ORD1326412320|[1326372660, 89.0...|ORD1326412320|[1326369060, 89.0...|ORD1326412320|[1326365460, 89.0...|[1326325860, 93.0...|[1326322260, 93.0...|[1326318660, 96.0...|[1326315060, 96.0...|[1326310980, 100....|[1326307860, 96.0...|[1326304260, 93.0...|[1326300660, 93.0...|[1326297060, 96.0...|[1326293460, 93.0...|[1326289860, 96.0...|[1326286260, 93.0...|\n",
      "|   ORD| CMH|ORD1327257000|CMH1327263300|1327257000| 1327213800|1327255200|1327251600|1327248000|1327244400|1327240800|1327237200|1327233600|1327230000|1327226400|1327222800| 1327219200| 1327215600| 1327212000|1327266360| 1327223160|1327263300|1327259700|1327256100|1327252500|1327248900|1327245300|1327241700|1327238100|1327234500|1327230900| 1327227300| 1327223700| 1327220100| true|ORD1327257000|[1327252320, 100....|ORD1327257000|[1327250820, 92.0...|ORD1327257000|[1327246020, 85.0...|ORD1327257000|[1327243860, 92.0...|ORD1327257000|[1327240140, 92.0...|ORD1327257000|[1327236660, 89.0...|ORD1327257000|[1327233060, 92.0...|ORD1327257000|[1327229460, 92.0...|ORD1327257000|[1327225860, 88.0...|ORD1327257000|[1327221780, 92.0...|ORD1327257000|[1327216860, 92.0...|ORD1327257000|[1327215060, 88.0...|[1327261860, 96.0...|[1327256880, 96.0...|[1327254660, 100....|[1327249380, 92.0...|[1327247040, 92.0...|[1327243860, 92.0...|[1327240260, 96.0...|[1327234800, 85.0...|[1327231320, 92.0...|[1327229460, 96.0...|[1327225860, 96.0...|[1327222260, 96.0...|\n",
      "|   TUL| DAL|TUL1326261420|DAL1326265500|1326261420| 1326218220|1326261600|1326258000|1326254400|1326250800|1326247200|1326243600|1326240000|1326236400|1326232800|1326229200| 1326225600| 1326222000| 1326218400|1326265380| 1326222180|1326265500|1326261900|1326258300|1326254700|1326251100|1326247500|1326243900|1326240300|1326236700|1326233100| 1326229500| 1326225900| 1326222300|false|TUL1326261420|[1326261180, 92.0...|TUL1326261420|[1326257580, 96.0...|TUL1326261420|[1326253980, 89.0...|TUL1326261420|[1326250380, 92.0...|TUL1326261420|[1326246780, 89.0...|TUL1326261420|[1326243180, 79.0...|TUL1326261420|[1326239580, 79.0...|TUL1326261420|[1326235980, 71.0...|TUL1326261420|[1326232380, 52.0...|TUL1326261420|[1326228780, 56.0...|TUL1326261420|[1326225180, 58.0...|TUL1326261420|[1326221580, 46.0...|[1326264780, 86.0...|[1326261180, 86.0...|[1326257580, 89.0...|[1326253980, 89.0...|[1326250380, 93.0...|[1326246780, 86.0...|[1326243180, 86.0...|[1326239580, 83.0...|[1326235980, 77.0...|[1326232380, 83.0...|[1326228780, 86.0...|[1326225180, 83.0...|\n",
      "|   GEG| DEN|GEG1326815520|DEN1326825900|1326815520| 1326772320|1326814500|1326810900|1326807300|1326803700|1326800100|1326796500|1326792900|1326789300|1326785700|1326782100| 1326778500| 1326774900| 1326771300|1326827280| 1326784080|1326825900|1326822300|1326818700|1326815100|1326811500|1326807900|1326804300|1326800700|1326797100|1326793500| 1326789900| 1326786300| 1326782700| true|GEG1326815520|[1326811860, 70.0...|GEG1326815520|[1326808380, 70.0...|GEG1326815520|[1326804780, 67.0...|GEG1326815520|[1326801180, 61.0...|GEG1326815520|[1326797460, 56.0...|GEG1326815520|[1326793980, 59.0...|GEG1326815520|[1326790380, 58.0...|GEG1326815520|[1326786780, 58.0...|GEG1326815520|[1326783180, 58.0...|GEG1326815520|[1326779580, 58.0...|GEG1326815520|[1326775980, 58.0...|GEG1326815520|[1326772380, 61.0...|[1326822780, 68.0...|[1326819180, 57.0...|[1326815580, 43.0...|[1326811980, 37.0...|[1326808380, 43.0...|[1326804780, 55.0...|[1326801180, 50.0...|[1326797580, 45.0...|[1326793980, 59.0...|[1326790380, 67.0...|[1326786780, 69.0...|[1326783180, 69.0...|\n",
      "+------+----+-------------+-------------+----------+-----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+-----------+-----------+-----------+----------+-----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+-----------+-----------+-----------+-----+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "workingDir: String = data/\n",
       "JT: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [ORIGIN: string, DEST: string ... 69 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "\n",
    "// Lecture parquet et conversion de Th du boolean vers string\n",
    "\n",
    "//val workingDir = \"/user/user388/dir_flight_hdfs/\"\n",
    "//val workingDir = \"\"\n",
    "val workingDir = \"data_parquet/\"\n",
    "\n",
    "val JT  = spark.read.parquet(workingDir + \"flight_1M.parquet\") // 1 mois de données = jan 2012 // V2 \"test_outputFull_1M_V2.parquet\"\"\n",
    ".withColumn(\"Th\",$\"Th\".cast(\"string\")).limit(1000)\n",
    "JT.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fde36f",
   "metadata": {},
   "source": [
    "## 3.2 - Balance the data to get same proportion for delaid and on time flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43476bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nb_total: Long = 1000\n",
       "nb_delay: Int = 502\n",
       "nb_onTime: Int = 498\n",
       "nb_selection: Int = 498\n",
       "JT_ok_shuffled: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [ORIGIN: string, DEST: string ... 69 more fields]\n",
       "JT_ok_shuffled_delay: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [ORIGIN: string, DEST: string ... 69 more fields]\n",
       "JT_ok_shuffled_onTime: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [ORIGIN: string, DEST: string ... 69 more fields]\n",
       "JT_balanced: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [ORIGIN: string, DEST: string ... 69 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// on compte le nbre total de lignes dans nos données\n",
    "val nb_total = JT.count\n",
    "\n",
    "// on compte le nbre de label en retard (ie 1.0)\n",
    "val nb_delay = JT.filter(JT(\"Th\") === \"true\").count.toInt\n",
    "\n",
    "// on compte le nbre de label à l'heure (ie 0.0)\n",
    "val nb_onTime = JT.filter(JT(\"Th\") === \"false\").count.toInt\n",
    "\n",
    "// on prend le plus petit nbre\n",
    "val nb_selection = Math.min(nb_onTime, nb_delay)\n",
    "\n",
    "// on mélange nos données\n",
    "val JT_ok_shuffled  = JT.orderBy(rand())\n",
    "\n",
    "// on prend autant en retard que on time\n",
    "val JT_ok_shuffled_delay=JT_ok_shuffled.filter(JT_ok_shuffled(\"Th\") === \"true\").limit(nb_selection)\n",
    "val JT_ok_shuffled_onTime=JT_ok_shuffled.filter(JT_ok_shuffled(\"Th\") === \"false\").limit(nb_selection)\n",
    "val JT_balanced = JT_ok_shuffled_delay.unionAll(JT_ok_shuffled_onTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4e1dc4",
   "metadata": {},
   "source": [
    "# 3.3 - Convert weather data array to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f80911a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "convertToString: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$4206/0x000000084167a840@4683592c,StringType,List(Some(class[value[0]: array<string>])),Some(class[value[0]: string]),None,true,true)\n",
       "JT_flatt: org.apache.spark.sql.DataFrame = [Th: string, ORIGIN: string ... 25 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val convertToString = udf((array: Seq[String]) => {array.mkString(\",\")})\n",
    "\n",
    "\n",
    "val JT_flatt = JT_balanced.na.drop()\n",
    "\n",
    "    .withColumn(\"slot1_D\", convertToString($\"slot1_D\"))\n",
    "    .withColumn(\"slot2_D\", convertToString($\"slot2_D\"))\n",
    "    .withColumn(\"slot3_D\", convertToString($\"slot3_D\"))\n",
    "    .withColumn(\"slot4_D\", convertToString($\"slot4_D\"))\n",
    "    .withColumn(\"slot5_D\", convertToString($\"slot5_D\"))\n",
    "    .withColumn(\"slot6_D\", convertToString($\"slot6_D\"))\n",
    "    .withColumn(\"slot7_D\", convertToString($\"slot7_D\"))\n",
    "    .withColumn(\"slot8_D\", convertToString($\"slot8_D\"))\n",
    "    .withColumn(\"slot9_D\", convertToString($\"slot9_D\"))\n",
    "    .withColumn(\"slot10_D\", convertToString($\"slot10_D\"))\n",
    "    .withColumn(\"slot11_D\", convertToString($\"slot11_D\"))\n",
    "    .withColumn(\"slot12_D\", convertToString($\"slot12_D\"))\n",
    "\n",
    "    .withColumn(\"slot1_A\", convertToString($\"slot1_A\"))\n",
    "    .withColumn(\"slot2_A\", convertToString($\"slot2_A\"))\n",
    "    .withColumn(\"slot3_A\", convertToString($\"slot3_A\"))\n",
    "    .withColumn(\"slot4_A\", convertToString($\"slot4_A\"))\n",
    "    .withColumn(\"slot5_A\", convertToString($\"slot5_A\"))\n",
    "    .withColumn(\"slot6_A\", convertToString($\"slot6_A\"))\n",
    "    .withColumn(\"slot7_A\", convertToString($\"slot7_A\"))\n",
    "    .withColumn(\"slot8_A\", convertToString($\"slot8_A\"))\n",
    "    .withColumn(\"slot9_A\", convertToString($\"slot9_A\"))\n",
    "    .withColumn(\"slot10_A\", convertToString($\"slot10_A\"))\n",
    "    .withColumn(\"slot11_A\", convertToString($\"slot11_A\"))\n",
    "    .withColumn(\"slot12_A\", convertToString($\"slot12_A\"))\n",
    "\n",
    "    .select(\"Th\", \"ORIGIN\", \"DEST\",\n",
    "            \"slot1_D\", \"slot2_D\", \"slot3_D\", \"slot4_D\", \"slot5_D\", \"slot6_D\", \"slot7_D\", \"slot8_D\", \"slot9_D\", \"slot10_D\", \"slot11_D\", \"slot12_D\",\n",
    "            \"slot1_A\", \"slot2_A\", \"slot3_A\", \"slot4_A\", \"slot5_A\", \"slot6_A\", \"slot7_A\", \"slot8_A\", \"slot9_A\", \"slot10_A\", \"slot11_A\", \"slot12_A\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a276a4d5",
   "metadata": {},
   "source": [
    "## 3.4 - Explode weather condition array to build new columns\n",
    "(for departure and arrival and the selected time slot)\n",
    "\n",
    "## Drop unused time slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b449c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JT_flat: org.apache.spark.sql.DataFrame = [Th: string, ORIGIN: string ... 55 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val JT_flat = JT_flatt\n",
    "    // niveau 1 du getItem\n",
    "    \n",
    "    .withColumn(\"S1D_Time\",split(col(\"slot1_D\"), \",\").getItem(0))\n",
    "    .withColumn(\"S1D_H\",split(col(\"slot1_D\"), \",\").getItem(1).cast(DoubleType))\n",
    "    .withColumn(\"S1D_Wd\",split(col(\"slot1_D\"), \",\").getItem(2).cast(DoubleType))\n",
    "    .withColumn(\"S1D_Ws\",split(col(\"slot1_D\"), \",\").getItem(3).cast(DoubleType))\n",
    "    .withColumn(\"S1D_P\",split(col(\"slot1_D\"), \",\").getItem(4).cast(DoubleType))\n",
    "    .withColumn(\"S1D_T6\",split(col(\"slot1_D\"), \",\").getItem(5).cast(DoubleType))\n",
    "    .withColumn(\"S1D_S\",split(col(\"slot1_D\"), \",\").getItem(6))\n",
    "    .withColumn(\"S1D_D\",split(col(\"slot1_D\"), \",\").getItem(7))\n",
    "    .withColumn(\"S1D_V\",split(col(\"slot1_D\"), \",\").getItem(8).cast(DoubleType))\n",
    "\n",
    "    .withColumn(\"S6D_Time\", split(col(\"slot6_D\"), \",\").getItem(0))\n",
    "    .withColumn(\"S6D_H\", split(col(\"slot6_D\"), \",\").getItem(1).cast(DoubleType))\n",
    "    .withColumn(\"S6D_Wd\", split(col(\"slot6_D\"), \",\").getItem(2).cast(DoubleType))\n",
    "    .withColumn(\"S6D_Ws\", split(col(\"slot6_D\"), \",\").getItem(3).cast(DoubleType))\n",
    "    .withColumn(\"S6D_P\", split(col(\"slot6_D\"), \",\").getItem(4).cast(DoubleType))\n",
    "    .withColumn(\"S6D_T6\", split(col(\"slot6_D\"), \",\").getItem(5).cast(DoubleType))\n",
    "    .withColumn(\"S6D_S\", split(col(\"slot6_D\"), \",\").getItem(6))\n",
    "    .withColumn(\"S6D_D\", split(col(\"slot6_D\"), \",\").getItem(7))\n",
    "    .withColumn(\"S6D_V\", split(col(\"slot6_D\"), \",\").getItem(8).cast(DoubleType))\n",
    "\n",
    "    .withColumn(\"S12D_Time\", split(col(\"slot12_D\"), \",\").getItem(0))\n",
    "    .withColumn(\"S12D_H\", split(col(\"slot12_D\"), \",\").getItem(1).cast(DoubleType))\n",
    "    .withColumn(\"S12D_Wd\", split(col(\"slot12_D\"), \",\").getItem(2).cast(DoubleType))\n",
    "    .withColumn(\"S12D_Ws\", split(col(\"slot12_D\"), \",\").getItem(3).cast(DoubleType))\n",
    "    .withColumn(\"S12D_P\", split(col(\"slot12_D\"), \",\").getItem(4).cast(DoubleType))\n",
    "    .withColumn(\"S12D_T6\", split(col(\"slot12_D\"), \",\").getItem(5).cast(DoubleType))\n",
    "    .withColumn(\"S12D_S\", split(col(\"slot12_D\"), \",\").getItem(6))\n",
    "    .withColumn(\"S12D_D\", split(col(\"slot12_D\"), \",\").getItem(7))\n",
    "    .withColumn(\"S12D_V\", split(col(\"slot12_D\"), \",\").getItem(8).cast(DoubleType))\n",
    "\n",
    "    .drop(\"slot1_D\",\"slot2_D\", \"slot3_D\", \"slot4_D\" , \"slot5_D\", \"slot6_D\", \"slot7_D\", \"slot8_D\", \"slot9_D\", \"slot10_D\", \"slot11_D\", \"slot12_D\")\n",
    "\n",
    "\n",
    "    .withColumn(\"S1A_Time\", split(col(\"slot1_A\"), \",\").getItem(0))\n",
    "    .withColumn(\"S1A_H\", split(col(\"slot1_A\"), \",\").getItem(1).cast(DoubleType))\n",
    "    .withColumn(\"S1A_Wd\", split(col(\"slot1_A\"), \",\").getItem(2).cast(DoubleType))\n",
    "    .withColumn(\"S1A_Ws\", split(col(\"slot1_A\"), \",\").getItem(3).cast(DoubleType))\n",
    "    .withColumn(\"S1A_P\", split(col(\"slot1_A\"), \",\").getItem(4).cast(DoubleType))\n",
    "    .withColumn(\"S1A_T6\", split(col(\"slot1_A\"), \",\").getItem(5).cast(DoubleType))\n",
    "    .withColumn(\"S1A_S\", split(col(\"slot1_A\"), \",\").getItem(6))\n",
    "    .withColumn(\"S1A_D\", split(col(\"slot1_A\"), \",\").getItem(7))\n",
    "    .withColumn(\"S1A_V\", split(col(\"slot1_A\"), \",\").getItem(8).cast(DoubleType))\n",
    "\n",
    "    .withColumn(\"S6A_Time\", split(col(\"slot6_A\"), \",\").getItem(0))\n",
    "    .withColumn(\"S6A_H\", split(col(\"slot6_A\"), \",\").getItem(1).cast(DoubleType))\n",
    "    .withColumn(\"S6A_Wd\", split(col(\"slot6_A\"), \",\").getItem(2).cast(DoubleType))\n",
    "    .withColumn(\"S6A_Ws\", split(col(\"slot6_A\"), \",\").getItem(3).cast(DoubleType))\n",
    "    .withColumn(\"S6A_P\", split(col(\"slot6_A\"), \",\").getItem(4).cast(DoubleType))\n",
    "    .withColumn(\"S6A_T6\", split(col(\"slot6_A\"), \",\").getItem(5).cast(DoubleType))\n",
    "    .withColumn(\"S6A_S\", split(col(\"slot6_A\"), \",\").getItem(6))\n",
    "    .withColumn(\"S6A_D\", split(col(\"slot6_A\"), \",\").getItem(7))\n",
    "    .withColumn(\"S6A_V\", split(col(\"slot6_A\"), \",\").getItem(8).cast(DoubleType))\n",
    "\n",
    "    .withColumn(\"S12A_Time\", split(col(\"slot12_A\"), \",\").getItem(0))\n",
    "    .withColumn(\"S12A_H\", split(col(\"slot12_A\"), \",\").getItem(1).cast(DoubleType))\n",
    "    .withColumn(\"S12A_Wd\", split(col(\"slot12_A\"), \",\").getItem(2).cast(DoubleType))\n",
    "    .withColumn(\"S12A_Ws\", split(col(\"slot12_A\"), \",\").getItem(3).cast(DoubleType))\n",
    "    .withColumn(\"S12A_P\", split(col(\"slot12_A\"), \",\").getItem(4).cast(DoubleType))\n",
    "    .withColumn(\"S12A_T6\", split(col(\"slot12_A\"), \",\").getItem(5).cast(DoubleType))\n",
    "    .withColumn(\"S12A_S\", split(col(\"slot12_A\"), \",\").getItem(6))\n",
    "    .withColumn(\"S12A_D\", split(col(\"slot12_A\"), \",\").getItem(7))\n",
    "    .withColumn(\"S12A_V\", split(col(\"slot12_A\"), \",\").getItem(8).cast(DoubleType))\n",
    "\n",
    "    .drop(\"slot1_A\", \"slot2_A\", \"slot3_A\",\"slot4_A\" , \"slot5_A\", \"slot6_A\", \"slot7_A\", \"slot8_A\", \"slot9_A\", \"slot10_A\", \"slot11_A\", \"slot12_A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b54c5b9",
   "metadata": {},
   "source": [
    "# 4 - Features indexation, Target indexation, Training/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb212d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Th: string (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- S1D_Time: string (nullable = true)\n",
      " |-- S1D_H: double (nullable = true)\n",
      " |-- S1D_Wd: double (nullable = true)\n",
      " |-- S1D_Ws: double (nullable = true)\n",
      " |-- S1D_P: double (nullable = true)\n",
      " |-- S1D_T6: double (nullable = true)\n",
      " |-- S1D_S: string (nullable = true)\n",
      " |-- S1D_D: string (nullable = true)\n",
      " |-- S1D_V: double (nullable = true)\n",
      " |-- S6D_Time: string (nullable = true)\n",
      " |-- S6D_H: double (nullable = true)\n",
      " |-- S6D_Wd: double (nullable = true)\n",
      " |-- S6D_Ws: double (nullable = true)\n",
      " |-- S6D_P: double (nullable = true)\n",
      " |-- S6D_T6: double (nullable = true)\n",
      " |-- S6D_S: string (nullable = true)\n",
      " |-- S6D_D: string (nullable = true)\n",
      " |-- S6D_V: double (nullable = true)\n",
      " |-- S12D_Time: string (nullable = true)\n",
      " |-- S12D_H: double (nullable = true)\n",
      " |-- S12D_Wd: double (nullable = true)\n",
      " |-- S12D_Ws: double (nullable = true)\n",
      " |-- S12D_P: double (nullable = true)\n",
      " |-- S12D_T6: double (nullable = true)\n",
      " |-- S12D_S: string (nullable = true)\n",
      " |-- S12D_D: string (nullable = true)\n",
      " |-- S12D_V: double (nullable = true)\n",
      " |-- S1A_Time: string (nullable = true)\n",
      " |-- S1A_H: double (nullable = true)\n",
      " |-- S1A_Wd: double (nullable = true)\n",
      " |-- S1A_Ws: double (nullable = true)\n",
      " |-- S1A_P: double (nullable = true)\n",
      " |-- S1A_T6: double (nullable = true)\n",
      " |-- S1A_S: string (nullable = true)\n",
      " |-- S1A_D: string (nullable = true)\n",
      " |-- S1A_V: double (nullable = true)\n",
      " |-- S6A_Time: string (nullable = true)\n",
      " |-- S6A_H: double (nullable = true)\n",
      " |-- S6A_Wd: double (nullable = true)\n",
      " |-- S6A_Ws: double (nullable = true)\n",
      " |-- S6A_P: double (nullable = true)\n",
      " |-- S6A_T6: double (nullable = true)\n",
      " |-- S6A_S: string (nullable = true)\n",
      " |-- S6A_D: string (nullable = true)\n",
      " |-- S6A_V: double (nullable = true)\n",
      " |-- S12A_Time: string (nullable = true)\n",
      " |-- S12A_H: double (nullable = true)\n",
      " |-- S12A_Wd: double (nullable = true)\n",
      " |-- S12A_Ws: double (nullable = true)\n",
      " |-- S12A_P: double (nullable = true)\n",
      " |-- S12A_T6: double (nullable = true)\n",
      " |-- S12A_S: string (nullable = true)\n",
      " |-- S12A_D: string (nullable = true)\n",
      " |-- S12A_V: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "labName: String = Th\n",
       "labelIndexer: org.apache.spark.ml.feature.StringIndexer = strIdx_3c34094b16c0\n",
       "featuresCols: Array[String] = Array(S1D_H, S1D_Wd, S1D_Ws, S1D_P, S1D_T6, S1D_V, S6D_H, S6D_Wd, S6D_Ws, S6D_P, S6D_T6, S6D_V, S12D_H, S12D_Wd, S12D_Ws, S12D_P, S12D_T6, S12D_V, S1A_H, S1A_Wd, S1A_Ws, S1A_P, S1A_T6, S1A_V, S6A_H, S6A_Wd, S6A_Ws, S6A_P, S6A_T6, S6A_V, S12A_H, S12A_Wd, S12A_Ws, S12A_P, S12A_T6, S12A_V)\n",
       "featureIndexer: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_22ebf5da6b03, handleInvalid=skip, numInputCols=36\n",
       "trainingData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Th: string, ORIGIN: string ... 55 more fields]\n",
       "testData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Th: string, ORIGIN: string ... 55 more fields]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/* ------------------------------------------  \n",
    "    Target indexation with StringIndexer\n",
    "-------------- ---------------------------- */\n",
    "val labName = \"Th\"\n",
    "val labelIndexer = new StringIndexer()\n",
    "    .setInputCol(labName)\n",
    "    .setOutputCol(\"indexed_\" + labName)\n",
    "    .setHandleInvalid(\"keep\") // ajout pour éviter l'erreur au moment du calcul d'accuracy\n",
    "\n",
    "/* ------------------------------------------  \n",
    "    Target indexation with VectorAssembler\n",
    "-------------- ---------------------------- */\n",
    "val featuresCols = Array(\"S1D_H\", \"S1D_Wd\",\"S1D_Ws\",\"S1D_P\",\"S1D_T6\",\"S1D_V\",\"S6D_H\", \"S6D_Wd\",\"S6D_Ws\",\"S6D_P\",\"S6D_T6\",\"S6D_V\", \"S12D_H\", \"S12D_Wd\",\"S12D_Ws\",\"S12D_P\",\"S12D_T6\",\"S12D_V\",\"S1A_H\", \"S1A_Wd\",\"S1A_Ws\",\"S1A_P\",\"S1A_T6\",\"S1A_V\",\"S6A_H\", \"S6A_Wd\",\"S6A_Ws\",\"S6A_P\",\"S6A_T6\",\"S6A_V\",\"S12A_H\", \"S12A_Wd\",\"S12A_Ws\",\"S12A_P\",\"S12A_T6\",\"S12A_V\") // sans S & D\n",
    "val featureIndexer = new VectorAssembler()\n",
    "    .setInputCols(featuresCols)\n",
    "    .setOutputCol(\"indexed_Features\")\n",
    "    .setHandleInvalid(\"skip\")\n",
    "\n",
    "/* ------------------------------------------  \n",
    "    Training/Test random Split\n",
    "-------------- ---------------------------- */\n",
    "val Array(trainingData, testData) = JT_flat.randomSplit(Array(0.7, 0.3), seed = 12345)\n",
    "trainingData.persist()\n",
    "\n",
    "trainingData.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f11dadf",
   "metadata": {},
   "source": [
    "## Helper function for performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0c22c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "performance: (model: org.apache.spark.ml.tuning.CrossValidatorModel, model_name: String)Any\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def performance(model : org.apache.spark.ml.tuning.CrossValidatorModel,model_name : String): Any = {\n",
    "\n",
    "            val training_predictions = model.transform(trainingData)\n",
    "                                  .select(\"prediction\", \"indexed_Th\")\n",
    "                                  .cache()\n",
    "\n",
    "            val test_predictions = model.transform(testData)\n",
    "                                  .select(\"prediction\", \"indexed_Th\")\n",
    "                                   .cache()\n",
    "    \n",
    "            val predictions = Array(training_predictions, test_predictions)\n",
    "            val names = Array(\" Training \", \" Test \")\n",
    "    \n",
    "    println(s\"\\n##################### ${model_name} Performance #########################\")\n",
    "    \n",
    "    for (i <- 0 until predictions.length)\n",
    "    {        \n",
    "         var data = predictions(i)\n",
    "         var name = names(i)\n",
    "    \n",
    "            // Select (prediction, true label) and compute test error.\n",
    "            var evaluator = new MulticlassClassificationEvaluator()\n",
    "              .setLabelCol(\"indexed_Th\")\n",
    "              .setPredictionCol(\"prediction\")\n",
    "              .setMetricName(\"accuracy\")\n",
    "              //.setMetricName(\"f1\")\n",
    "\n",
    "            var accuracy = evaluator.evaluate(data)\n",
    "\n",
    "\n",
    "    \n",
    "            evaluator = new MulticlassClassificationEvaluator()\n",
    "              .setLabelCol(\"indexed_Th\")\n",
    "              .setPredictionCol(\"prediction\")\n",
    "              .setMetricName(\"f1\")\n",
    "\n",
    "            var f1 = evaluator.evaluate(data)\n",
    "            \n",
    "            \n",
    "            println(s\"\\n---------------------- ${name} performance metrics----------------------\\n\")\n",
    "            println(s\"\\t- Accuracy = ${(accuracy * 100)}\")\n",
    "            println(s\"\\t- Error = ${(1.0 - accuracy)*100}\")\n",
    "            println(s\"\\t- F1 score = ${(f1)}\")\n",
    "    }\n",
    "      \n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daf5c3d",
   "metadata": {},
   "source": [
    "# 5 - Random Forest : build pipeline, train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70237036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\trfc_769ca594add6-bootstrap: true,\n",
      "\trfc_769ca594add6-cacheNodeIds: false,\n",
      "\trfc_769ca594add6-checkpointInterval: 10,\n",
      "\trfc_769ca594add6-featureSubsetStrategy: auto,\n",
      "\trfc_769ca594add6-featuresCol: indexed_Features,\n",
      "\trfc_769ca594add6-impurity: entropy,\n",
      "\trfc_769ca594add6-labelCol: indexed_Th,\n",
      "\trfc_769ca594add6-leafCol: ,\n",
      "\trfc_769ca594add6-maxBins: 200,\n",
      "\trfc_769ca594add6-maxDepth: 20,\n",
      "\trfc_769ca594add6-maxMemoryInMB: 256,\n",
      "\trfc_769ca594add6-minInfoGain: 0.0,\n",
      "\trfc_769ca594add6-minInstancesPerNode: 1,\n",
      "\trfc_769ca594add6-minWeightFractionPerNode: 0.0,\n",
      "\trfc_769ca594add6-numTrees: 200,\n",
      "\trfc_769ca594add6-predictionCol: prediction,\n",
      "\trfc_769ca594add6-probabilityCol: probability,\n",
      "\trfc_769ca594add6-rawPredictionCol: rawPrediction,\n",
      "\trfc_769ca594add6-seed: 207336481,\n",
      "\trfc_769ca594add6-subsamplingRate: 1.0\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rf: org.apache.spark.ml.classification.RandomForestClassifier = rfc_769ca594add6\n",
       "steps_rf: Array[org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable}}] = Array(strIdx_3c34094b16c0, VectorAssembler: uid=vecAssembler_22ebf5da6b03, handleInvalid=skip, numInputCols=36, rfc_769ca594add6)\n",
       "pipeline_rf: org.apache.spark.ml.Pipeline = pipeline_9c7bd8212e59\n",
       "paramGrid_rf: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\trfc_769ca594add6-impurity: entropy,\n",
       "\trfc_769ca594add6-maxBins: 200,\n",
       "...\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//########## instanciate Model\n",
    "val rf = new RandomForestClassifier()\n",
    "  .setLabelCol(\"indexed_Th\")\n",
    "  .setFeaturesCol(\"indexed_Features\")\n",
    "\n",
    "//########## define pipeline steps\n",
    "val steps_rf = Array(labelIndexer, featureIndexer, rf)\n",
    "\n",
    "//########## create pipeline with steps and model\n",
    "val pipeline_rf = new Pipeline().setStages(steps_rf)\n",
    "\n",
    "//########## Set grid search\n",
    "val paramGrid_rf = new ParamGridBuilder()\n",
    "  //.addGrid(vectorIndexer.maxCategories, Array(18))\n",
    "  .addGrid(rf.maxBins, Array(200))\n",
    "  .addGrid(rf.maxDepth, Array(20))\n",
    "  .addGrid(rf.numTrees, Array(200))\n",
    "  .addGrid(rf.impurity, Array(\"entropy\"))//, \"gini\"))\n",
    "  .build()\n",
    "\n",
    "//########## Set evaluator for crossvalidation\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"indexed_Th\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "\n",
    "//########## Set 3 folds crossvalidator with pipeline and \n",
    "val crossval_rf = new CrossValidator()\n",
    "    .setEstimator(pipeline_rf)\n",
    "    .setEvaluator(evaluator)\n",
    "    .setEstimatorParamMaps(paramGrid_rf)\n",
    "    .setNumFolds(3)\n",
    "\n",
    "//########## Fit the model\n",
    "val modelRF_Grid = crossval_rf.fit(trainingData)\n",
    "\n",
    "//########## Print model best parameters\n",
    "println(\"######################## Model best parameters ########################\")\n",
    "println(modelRF_Grid.bestModel.asInstanceOf[PipelineModel].stages.last.extractParamMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8141dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##################### Random Forest Performance #########################\n",
      "\n",
      "----------------------  Training  performance metrics----------------------\n",
      "\n",
      "\t- Accuracy = 100.0\n",
      "\t- Error = 0.0\n",
      "\t- F1 score = 1.0\n",
      "\n",
      "----------------------  Test  performance metrics----------------------\n",
      "\n",
      "\t- Accuracy = 76.26459143968872\n",
      "\t- Error = 23.73540856031129\n",
      "\t- F1 score = 0.7628620018733485\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res2: Any = ()\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance(modelRF_Grid,\"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "30858c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "/*import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator  \n",
    "import org.apache.spark.mllib.evaluation.MulticlassMetrics  \n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics*/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee16a1d",
   "metadata": {},
   "source": [
    "# 6 - GBT : build pipeline, train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "060397eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gbt: org.apache.spark.ml.classification.GBTClassifier = gbtc_c19788c54ac3\n",
       "steps_gbt: Array[org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable}}] = Array(strIdx_3c34094b16c0, VectorAssembler: uid=vecAssembler_22ebf5da6b03, handleInvalid=skip, numInputCols=36, gbtc_c19788c54ac3)\n",
       "pipeline_gbt: org.apache.spark.ml.Pipeline = pipeline_64e3de9216b2\n",
       "paramGrid_gbt: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tgbtc_c19788c54ac3-maxBins: 200,\n",
       "\tgbtc_c19788c54ac3-maxDepth: 20,\n",
       "\tgbtc_...\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//########## instanciate Model\n",
    "val gbt = new GBTClassifier()\n",
    "  .setLabelCol(\"indexed_Th\")\n",
    "  .setFeaturesCol(\"indexed_Features\")\n",
    "  .setFeatureSubsetStrategy(\"auto\")\n",
    "\n",
    "//########## define pipeline steps\n",
    "val steps_gbt = Array(labelIndexer, featureIndexer, gbt)\n",
    "\n",
    "//########## create pipeline with steps and model\n",
    "val pipeline_gbt = new Pipeline().setStages(steps_gbt)\n",
    "\n",
    "//########## Set grid search\n",
    "val paramGrid_gbt = new ParamGridBuilder()\n",
    "  //.addGrid(vectorIndexer.maxCategories, Array(18))\n",
    "  .addGrid(gbt.maxBins, Array(200))\n",
    "  .addGrid(gbt.maxIter, Array(50))\n",
    "  .addGrid(gbt.maxDepth, Array(20))\n",
    "  //.addGrid(gbt.numTrees, Array(25))\n",
    "  //.addGrid(gbt.impurity, Array(\"gini\"))//, \"gini\"))\n",
    "  .build()\n",
    "\n",
    "//########## Set 3 folds crossvalidator with pipeline and \n",
    "val crossval = new CrossValidator()\n",
    "    .setEstimator(pipeline_gbt)\n",
    "    .setEvaluator(evaluator)\n",
    "    .setEstimatorParamMaps(paramGrid_gbt)\n",
    "    .setNumFolds(3)\n",
    "\n",
    "//########## Fit the model\n",
    "val model_gbt_Grid = crossval.fit(trainingData)\n",
    "\n",
    "//########## Print model best parameters\n",
    "println(\"######################## Model best parameters ########################\")\n",
    "println(model_gbt_Grid.bestModel.asInstanceOf[PipelineModel].stages.last.extractParamMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfe7305f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##################### Gradient Boosted Tree Performance #########################\n",
      "\n",
      "----------------------  Training  performance metrics----------------------\n",
      "\n",
      "\t- Accuracy = 100.0\n",
      "\t- Error = 0.0\n",
      "\t- F1 score = 1.0\n",
      "\n",
      "----------------------  Test  performance metrics----------------------\n",
      "\n",
      "\t- Accuracy = 67.31517509727627\n",
      "\t- Error = 32.68482490272373\n",
      "\t- F1 score = 0.6734097368402401\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res4: Any = ()\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance(model_gbt_Grid,\"Gradient Boosted Tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12156764",
   "metadata": {},
   "source": [
    "# 7 - Logistic Regression : build pipeline, train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "037498d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_28a8c5f32d7c\n",
       "steps_lr: Array[org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable}}] = Array(strIdx_3c34094b16c0, VectorAssembler: uid=vecAssembler_22ebf5da6b03, handleInvalid=skip, numInputCols=36, logreg_28a8c5f32d7c)\n",
       "pipeline_lr: org.apache.spark.ml.Pipeline = pipeline_aaca8cfda3d9\n",
       "paramGrid_lr: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tlogreg_28a8c5f32d7c-elasticNetParam: 0.8,\n",
       "\tlogreg_28a8c5f32d7c-max...\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//########## instanciate Model\n",
    "val lr = new LogisticRegression()\n",
    "  .setLabelCol(\"indexed_Th\")\n",
    "  .setFeaturesCol(\"indexed_Features\")\n",
    "\n",
    "//########## define pipeline steps\n",
    "val steps_lr = Array(labelIndexer, featureIndexer, lr)\n",
    "\n",
    "//########## create pipeline with steps and model\n",
    "val pipeline_lr = new Pipeline().setStages(steps_lr)\n",
    "\n",
    "//########## Set grid search\n",
    "val paramGrid_lr = new ParamGridBuilder()\n",
    "  .addGrid(lr.regParam, Array(0.3))\n",
    "  .addGrid(lr.maxIter, Array(50))\n",
    "  .addGrid(lr.elasticNetParam, Array(0.8))\n",
    "  .build()\n",
    "\n",
    "//########## Set 3 folds crossvalidator with pipeline and \n",
    "val crossval = new CrossValidator()\n",
    "    .setEstimator(pipeline_lr)\n",
    "    .setEvaluator(evaluator)\n",
    "    .setEstimatorParamMaps(paramGrid_lr)\n",
    "    .setNumFolds(3)\n",
    "\n",
    "\n",
    "//########## Fit the model\n",
    "val model_lr_Grid = crossval.fit(trainingData)\n",
    "\n",
    "//########## Print model best parameters\n",
    "println(\"######################## Model best parameters ########################\")\n",
    "println(model_lr_Grid.bestModel.asInstanceOf[PipelineModel].stages.last.extractParamMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4faa6e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##################### Logistic Regression Performance #########################\n",
      "\n",
      "----------------------  Training  performance metrics----------------------\n",
      "\n",
      "\t- Accuracy = 51.64319248826291\n",
      "\t- Error = 48.35680751173709\n",
      "\t- F1 score = 0.35174929868166693\n",
      "\n",
      "----------------------  Test  performance metrics----------------------\n",
      "\n",
      "\t- Accuracy = 46.69260700389105\n",
      "\t- Error = 53.30739299610895\n",
      "\t- F1 score = 0.29724736554201203\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res5: Any = ()\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance(model_lr_Grid,\"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b900677",
   "metadata": {},
   "source": [
    "# 8 - Grid Search on Random Forest (the best model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f7f9305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################## Model best parameters ########################\n",
      "{\n",
      "\trfc_769ca594add6-bootstrap: true,\n",
      "\trfc_769ca594add6-cacheNodeIds: false,\n",
      "\trfc_769ca594add6-checkpointInterval: 10,\n",
      "\trfc_769ca594add6-featureSubsetStrategy: auto,\n",
      "\trfc_769ca594add6-featuresCol: indexed_Features,\n",
      "\trfc_769ca594add6-impurity: gini,\n",
      "\trfc_769ca594add6-labelCol: indexed_Th,\n",
      "\trfc_769ca594add6-leafCol: ,\n",
      "\trfc_769ca594add6-maxBins: 100,\n",
      "\trfc_769ca594add6-maxDepth: 10,\n",
      "\trfc_769ca594add6-maxMemoryInMB: 256,\n",
      "\trfc_769ca594add6-minInfoGain: 0.0,\n",
      "\trfc_769ca594add6-minInstancesPerNode: 1,\n",
      "\trfc_769ca594add6-minWeightFractionPerNode: 0.0,\n",
      "\trfc_769ca594add6-numTrees: 200,\n",
      "\trfc_769ca594add6-predictionCol: prediction,\n",
      "\trfc_769ca594add6-probabilityCol: probability,\n",
      "\trfc_769ca594add6-rawPredictionCol: rawPrediction,\n",
      "\trfc_769ca594add6-seed: 207336481,\n",
      "\trfc_769ca594add6-subsamplingRate: 1.0\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "paramGrid_rf: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\trfc_769ca594add6-impurity: entropy,\n",
       "\trfc_769ca594add6-maxBins: 100,\n",
       "\trfc_769ca594add6-maxDepth: 10,\n",
       "\trfc_769ca594add6-numTrees: 100\n",
       "}, {\n",
       "\trfc_769ca594add6-impurity: entropy,\n",
       "\trfc_769ca594add6-maxBins: 150,\n",
       "\trfc_769ca594add6-maxDepth: 10,\n",
       "\trfc_769ca594add6-numTrees: 100\n",
       "}, {\n",
       "\trfc_769ca594add6-impurity: entropy,\n",
       "\trfc_769ca594add6-maxBins: 200,\n",
       "\trfc_769ca594add6-maxDepth: 10,\n",
       "\trfc_769ca594add6-numTrees: 100\n",
       "}, {\n",
       "\trfc_769ca594add6-impurity: gini,\n",
       "\trfc_769ca594add6-maxBins: 100,\n",
       "\trfc_769ca594add6-maxDepth: 10,\n",
       "\trfc_769ca594add6-numTrees: 100\n",
       "}, {\n",
       "\trfc_769ca594add6-impurity: gini,\n",
       "\trfc_769ca594add6-maxBins: 150,\n",
       "\trfc_769ca594add6-maxDepth: 10,\n",
       "\trfc_769ca594add6-numTrees: 100\n",
       "}, {\n",
       "\trfc_769ca594add6-impurity: gini,\n",
       "\trfc_769ca594...\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//########## Set grid search\n",
    "val paramGrid_rf = new ParamGridBuilder()\n",
    "  //.addGrid(vectorIndexer.maxCategories, Array(18))\n",
    "  .addGrid(rf.maxBins, Array(100,150,200,250))\n",
    "  .addGrid(rf.maxDepth, Array(10,20,30))\n",
    "  .addGrid(rf.numTrees, Array(100,150,200))\n",
    "  .addGrid(rf.impurity, Array(\"entropy\",\"gini\"))\n",
    "  .build()\n",
    "\n",
    "//########## Set evaluator for crossvalidation\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"indexed_Th\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "\n",
    "//########## Set 3 folds crossvalidator with pipeline and \n",
    "val crossval_rf = new CrossValidator()\n",
    "    .setEstimator(pipeline_rf)\n",
    "    .setEvaluator(evaluator)\n",
    "    .setEstimatorParamMaps(paramGrid_rf)\n",
    "    .setNumFolds(3)\n",
    "\n",
    "//########## Fit the model\n",
    "val model = crossval_rf.fit(trainingData)\n",
    "\n",
    "//########## Print model best parameters\n",
    "println(\"######################## Model best parameters ########################\")\n",
    "println(model.bestModel.asInstanceOf[PipelineModel].stages.last.extractParamMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371ef44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance(model,\"Random Forest\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
